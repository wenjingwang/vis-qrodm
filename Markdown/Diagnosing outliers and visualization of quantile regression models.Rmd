---
title: "Diagnosing outliers and visualization of quantile regression models"
author: 
  - Wenjing Wang^1^, Dianne Cook^2^, Earo Wang^2^
  - ^1^Renmin University of China  , ^2^Monash University
---

# Introduction

## Background of quantile regression

Quantile regression model has been widely used in many research areas such as economy, finance, social science and medicine. In quantile regression, observed covariate described the distribution of response variable which extend the mean regression analysis. In addition, mean regression can no longer maintain the optimal properties due to heteroscedasticity or heavy tail distribution. 

Linear quantile regression model can be present as $y=X^{'}\beta+\epsilon$, where $y$ is response variable, $X$ is covariate variable vector, $\beta$ and $\epsilon$ are coefficient vector and error term. The $\tau$th quantile function of the sample is $Q_{y}(\tau|x)=X^{'}\beta(\tau)$. Based on the idea of minimizing a sum of asymmetrically weighted absolute residuals, the objective function of quantile regression model is:

$$\min\sum_{i=1}^{n} \rho_{\tau}(y_i-X^{'}\beta)$$

where $\rho(.)$ is loss function which was defined as $\rho_{\tau}(u)=u(\tau-I(u <0))$

Assuming $Y_1,...,Y_n$ is a sequence of i.i.d random variables, which has distribution function $F$ and continuous density function $f$. The objective function and gradient function on $\tau$th quantile are $Q_{\tau}$ and $g_{n}(Q_{\tau})$.

mean: $Eg_{n}(Q_{\tau}\tau+\delta/n^{1/2}) ... f(Q_{\tau})\delta/n^{1/2}$
variance??$V(g_{n}(Q_{\tau}+\delta/n^{1/2})) ... \tau(1-\tau)/n$

Considering the goodness of model fitting, if the distribution of error term are known and i.i.d, we can follow the traditional diagnostic method by compareing the value of maximum likelihood function of constrained and unconstrained model. They should be significantly close enough.

Assuming the null hypothesis is $H_{0}: R\beta = r$.

$$s(\tau)=[f(F^{-1}(\tau))]^{-1},\\
\hat{V}(\tau)=\min_{\beta \in R^{p}} \sum \rho_{\tau}(y_{i}-x^{'}_{i}\beta),\\
\hat{V}(\tau)=\min_{b \in R^{p} | R\beta=r} \sum \rho_{\tau}(y_{i}-x^{'}_{i}\beta)$$

When null hypothesis is true, the testing statistics $T_n$ approximately follows $chisq^[2](q)$ distribution. One step further, the testing statistics of $\tau$th quantile regression is:

$$T_{n}=\frac{2}{\lambda^{2}(\tau)s(\tau)}(V(\tau)-V(\hat{\tau}))$$

where, $\lambda^{2}(\tau)=\tau(1-\tau)$.


## Outlier detection in regression and HLMdiag

Sample data from the real world may have special points located far away from others. In regression model, these points may affect model fitting. In single variabe case, we can observe outliers based on scatter plot. Difficulty lies in high-dimensional situation, where statistical methods should be used.

Outliers in regression can be divided into two class, one is outliers in vertical direction and the other is leverage point. Various methods for detecting outliers have been studied(Atkinson 1994; Barnett and Lewis 1994; Becker and Gather 1999, 2001; Davies and Gather 1993; Gather and Becker 1997; Gnanadesikan and Kettenking 1972; Hadi 1992, 1994; Hawkins 1980; Maronna and Yohai 1995; Penny 1995; Rocke and Woodruff 1996; Rousseeuw and Van Zomeren 1990). Commonly used methods in mean regression including residuals, leverage value, studentized residuals and jacknife residuals.

Classic least ordinary square estimation of linear regresssion can be expressed as $\hat{\beta}=(X^{'}X)^{-1}X^{'}Y$??$\hat{Y}=X(X^{'}X)^{-1}X^{'}Y=HY$, where, $H$ is called hat matrix. Residuals can be write as $\hat{\epsilon}=Y-\hat{Y}(1-H)Y=(1-H)\epsilon$. The variance of the error term and the estimation of $Y$ are  $Var(\hat{\epsilon})=(1-H)Var(\epsilon)=(1-H)\sigma^{2}$, $Var(\hat{Y})=X\hat{\beta}=H\sigma^{2}$. 

If taking outliers in $y$ and leverage points all in consideration, we can construct studentized residuals, which is $r_i=\frac{\hat{epsilon}_{i}}{\sigma^{2}\sqrt{1-h_i}$. The larger $r_i$, the more suspicious the outlier is.

Another widely used outlier diagnositc idea is `leave-one-out`. Jackknif residual, knowing as $t_i=\frac{\hat{y_{(i)}-\hat{y_i}}{\hat{\sigma}_{(i)}(1+x^{'}_{i}(X^{'}_{(i)}X_{(i)})^{-1}x_{i})^{-1/2}}$ and cook distance used this idea.



# Outlier detection in quantile regression

Since Quantile regressions are relatively robust towards outliers than mean regression model, few diagnostic methods discussed to check the stability or robustness of them. This property has been discussed by Onyedikachi(2015). They used influence funtion to discuss the property of robustness of quantile regression. 

Suppose $T$ is a function of $F$, the influence function is the directional derivative of $T(F)$ at $F$, and it measures the effect of a small perturbation in $F$ on $T(F)$. For the $\tau$th quantile points, influence function can be expressed as,

\begin{equation}

IF(y;T;F)=\left\{
\begin{aligned}
\frac{\tau}{f(F^{-1}(\tau))} & ; & y > F^{-1}(\tau) \\
\frac{(\tau-1)}{f(F^{-1}(\tau))} & ; & y \leq F^{-1}(\tau) 
\end{aligned}
\right.

(\#eq:quantile_influence)
\end{equation}

The influence function \@ref(eq:quantile_influence) indicate that the contamination in $y$ on quantile points is bounded. Onyedikachi(2015) also provided the influence function for quantile regression.

Let $F$ represent the joint distribution of the pairs $(x,y)$, we have,

\begin{equation}
IF((y,x),\hat{\beta}_{F(\tau)},F)=Q^{-1}xsgn(y-x^{'}\hat{\beta}_{F}(\tau))
(\#eq:quantile_regression_influence)
\end{equation}

where 

\begin{equation}
dF=dG(x)f(y|x)dy
(\#eq: dg)
\end{equation}

\begin{equation}
Q=\int xx^{'}f(X^{'}\hat{\beta}_{F}(\tau))dG(x)
(\#eq: q_influence)
\end{equation}

\@ref(eq:quantile_regression_influence) implies that in quantile regression, the quantile regression estimates will not be affected by any chagne in the value of the dependent variable for some observations as long as the relative positions of the observation points to the fitted plane are maintained. 

## Displaying how do the outliers affect quantile regression

Simulation study

```{r, message = FALSE}
library(quokar)
library(quantreg)
library(tidyverse)
```

In conclusion, quantile regression response differently to outliers comparing mean regression in two aspects: (a) not all models on each quantile will be affected when outliers exist. If we are interested in model on particular quantile, the effect of outliers should be carefully considered. (b) quantile regression model do not have robustness properties to so called leverage points. Koenker(2017) also proposed that more work needs to be done to develop better diagnostic tools for quantile regression models.

## Diagnosting methods

 However, leverage points can not be detected using the famous "Hat Matrix", since the coefficient estimation of quantile regression do not satisfy $\hat{\beta}=(X^{'}X)^{-1}X^{'}Y$.

We proposed three methods for quantile regression outlier diagnostic which will be discussed as follows.

* Standard residual-Robust Distance

One way to identify possible multivariate outliers is to calculate a distance from each point to a "center" of the data. An outlier would then be a point with a distance larger than some predetermined cutoff. A conventional measurement of quadradic distance from a point $X$ to a location $Y$ given given a shape $S$, in the multivariate setting is:

$$d^{2}_{S}(X,Y)=(X-Y)^{'}S^{-1}(X-Y)$$

This distance is often called Mahalanobis squared distance(MSD). If there are only a few outliers, large values of $d^{2}_{S}(x_i, \bar{X})$, when $\bar{X}$ and $S$ are the standard sample mean and covariance matrix, indicate that the point $x_i$ is an outlier(Barnett and Lewis 1994). The distribution of the MSD with both the true location and shape parameters and the standard sample location and shape parameters is well known(Gnanadesikan and Kettenring 1972). However, the standard sample locatoin and shape parameters are not robust to outliers, and the distributional fit to the distance breaks down when robust measures of location and shape are used in the MSD(Rousseeuw and van Zomeren 1991). Datasets with multiple outliers or clusters of outliers are subject to problems of masking and swamping(Pearson and Chandra Sekar 1936).

Problems of masking and swamping can be resolved by using robust estimates of shape and location, which by definition are less affected by outliers. We use Rousseeuw's minimum covariance determinant(MCD)(Rousseeuw 1985) to estimate the location and shape of the data. 

The MCD estimator is a robust, high-breakdown point which can be defined as:

$$MCD = (\bar{X}^{*}_{h}, S^{*}_{h})$$

where $h={p: |S^{*}_{h}|<|S^{*}_{k}|, |k|=p}$, $\bar{X}^{*}_{h}=\frac{1}{p}\sum_{i \in p} x_{i}$, $S^{*}_{p}=\frac{1}{p}\sum_{i \in p} (x_i-\bar{X}^{*}_{p})(x_i-\bar{X}^{*}_{p})^{'}$.

The value $p$ can be thought of as the minimum number of points which must not be outliers. The MCD has its highest possible breakdown at $h=[\frac{n+p+1}{2}]$ where $[.]$ is the greatest integer function. Because we are interested in outlier detection, we will use $h$ at its highest possible breakdown. $h=[\frac{n+p+1}{2}]$ in our calculations, and we refer to a sample of size $h$ as a "half sample" The MCD is omputed from the "closet" half sample, and therefore, the outlying points will have little affect on the MCD location or shape estimate.

We use MCD to detect outliers in covariates and use absolute standard residuals to detect outliers in y direction.
 
* Generalized Cook Distance

To assess the influence of the $i$th case on the coefficient estimation of quantile regression, we compare the difference between $\hat{\theta}_{[i]}$ and $\hat{\theta}$.

Case-deletion is a classical approach to study the effects of dropping the $i$th observation deleted. Thus, the complete-data log-likelihood function based on the data with the $i$th case deleted will be denoted by $L_{c}(\theta|y_{c[i]})$. Let $\hat{\theta}_{[i]}=(\hat{\beta}^{T}_{p[i]}, \hat{\sigma}^{2}_{[i]})^{T}$ be the maximizer of the function $Q_{[i]}(\theta|\hat{\theta})=E_{\hat{\theta}}[l_{c}(\theta|Y_{c[i]})|y]$, where $\hat{\theta}=(\hat{\beta}^{T}, \hat{\sigma}^{2})^{T}$ is the ML estimate of $\theta$. 

To calculate the case-deletion estimate $\hat{\theta}_{[i]}$ of $\theta$, proposed the following one-step approximation based on Q-function,

$$\hat{\theta}_{[i]}=\hat{\theta}+\{-Q(\hat{\theta}|\hat{\theta})\}^{-1}Q_{[i]}(\hat{\theta}|\hat{\theta})$$

where

$$Q(\hat{\theta}|\hat{\theta})=\frac{\partial^{2}Q(\theta|\hat{\theta})}{\partial\theta\partial \theta^{T}}|_{\theta=\hat{\theta}}$$

$$Q_{[i]}(\hat{\theta}|\hat{\theta})=\frac{\partial Q_{[i]}(\theta|\hat{\theta})}{\partial\theta}|_{\theta=\hat{\theta}}$$


are the Hessian matrix and the gradient vector evaluated at $\hat{\theta}$, respectively.

For measuring the distance between $\hat{\theta}_{[i]}$ and $\hat{\theta}$. We consider generalized cook distance as follows.

$$GD_{i} =(\hat{\theta}_{[i]}-\hat{\theta})^{T}\{-Q(\hat{\theta}|\hat{\theta})\}(\hat{\theta}_{[i]}-\hat{\theta}), i=1,...,n$$


* Q-function Distance

The measurement of the influence of the $i$th case is based on the Q-distance function, similar to the likelihood distance $LD_{i}$ which was defined as

$$QD_{i}=2\{Q(\hat{\theta}|\hat{\theta})-Q(\hat{\theta}_{[i]}|\hat{\theta})\}$$

* Mean Post Probability 

Baysian quantile regression added a latent variable $v_i$ into model for each observation. Every $v_i$ is assumed to have an exponential distribution with mean $\sigma$, that with the likelihood produces a posterior distributed according to a generalized inverse Gaussian with parameters.

If we define the variable $O_i$, which takes value equal to 1 when the $i$th observation is an outlier, and 0 otherwise. Then we propose to calculate the pro
bility of an observation being an outlier as

$$P(O_i=1)=\frac{1}{n-1}\sum_{j \neq i}P(v_i > v_j|data)$$

The probability in the expression above can be approximated given the
MCMC draws, as follows:

$$P(O_i = 1)=\frac{1}{M}I(v^{(l)_i}>max_{k \in 1:M}v^{(k)}_j)$$

where $M$ is the size of the chain of $v_i$ after the burn-in perior and $v^{(l)}_i$ is the $l$th draw of this chain.


* K-L Divergence

Similar with mean posterior probability method, we also caculates Kullback-Leibler divergence which proposed by Kullback and Leibler(1951) as a more precise method of measuring the distance between those latent variables in Bayes quantile regression. The Kullback-Leibler divergence is defined as:
  
$$K(f_i, f_j)=\int log(\frac{f_i(x)}{f_j(x)}f_{i}(x))dx$$
  
where $f_i$ could be the posterior conditional distribution of $v_i$ and $f_j$ the posterior conditional distribution of $v_j$. We should average this divergence for one observation based on the distance from all others,
  
$$KL(f_i)=\frac{1}{n-1}\sum_{j\neq i}K(f_i, f_j)$$
  
The outliers should show a high probability value for this divergence. We compute the integral using the trapezoidal rule.

# Examining outlier detection

We developed R package `quokar` to implete quantile regression outlier diagnostic methods. This package mainly realized two basic features: (a) plot the outlier states; (b) plot data with outliers marked. `quokar` is available from Github at https://github.com/wenjingwang/quokar, so to install and load withn R use:

```{r}
devtools::install_github("wenjingwang/quokar")
library(quokar)
```

We implete ais data as an example to introduce this package. AIS data include 14 variables for 100 female atheletes.

## Plot the outlier stats

In single variable case, we can use scatter plot to represent the outlier stats. The following code showed how to display suspicious outliers based on quantile regression models.

```{r}
data(ais)
ais_female <- filter(ais, Sex == 1)
case <- 1 : nrow(ais_female)
ais_female <- cbind(case, ais_female)
coef_rq <- coef(rq(BMI ~ LBM, tau = c(0.1, 0.5, 0.9),
                   data = ais_female, method = "br"))

br_coef <- data.frame(intercept = coef_rq[1, ],
                      coef = coef_rq[2, ],
                      tau_flag = colnames(coef_rq))
ggplot(ais_female)+
  geom_point(aes(x = LBM, y = BMI)) +
  geom_abline(data = br_coef, aes(intercept = intercept,
                                  slope = coef,
                                  colour = tau_flag), size = 1) +
  geom_text(data = subset(ais_female, case %in% c(1, 75)),
                          aes(x = LBM, y = BMI, label = case), 
            colour = "red",hjust = 0, vjust = 0) +
  scale_colour_brewer("Dark2") +
  theme_dark()

```

## Plot data with outliers marked

Scatter plot has limitations when tackling multi-variable regression cases. In `quokar`, we provide functions to do outlier diagnostic which return the dataframe easily to plot data with outliers marked.

* residual-robust distance method

First, we calculate residuals, mahananobi distance and robust distance for quantile regression using function `plot_distance`. Simutaneously, it provides the cutoff value for identifying the outliers in regression models.

```{r}
tau <- c(0.1, 0.5, 0.9)
object <- rq(BMI ~ LBM + Bfat, data = ais_female, tau = tau)
plot_distance <- frame_distance(object, tau = c(0.1, 0.5, 0.9))
distance <- plot_distance[[1]]
head(distance, 3)
cutoff_v <- plot_distance[[2]]; cutoff_v
cutoff_h <- plot_distance[[3]]; cutoff_h
```

Function `plot_distance` returns the tidy data form for plotting data with outliers marked and overlaying the cutoff lines. 

```{r Residual-Robust,warning=FALSE,message=FALSE, fig.height=3, fig.width=8, fig.align = "center", fig.cap="Robust Distance-Residual Plot. Points on the right of vertical cutoff line are considered leverage points and points above the horizental cutoff line are outliers in y-direction."}
n <- nrow(object$model)
case <- rep(1:n, length(tau))
distance <- cbind(case, distance)
distance$residuals <- abs(distance$residuals)
tau_flag <- paste("tau", tau, sep="")
text_flag <- 1:length(cutoff_h) %>%
                            map(function(i){
                            distance %>% 
                            filter((residuals > cutoff_h[i] |rd > cutoff_v)
                                & tau_flag == tau_flag[i])})
##need polish
text_flag_d <- rbind(text_flag[[1]], text_flag[[2]], text_flag[[3]])
ggplot(distance, aes(x = rd, y = residuals)) +
      geom_point() +
      geom_hline(data = data.frame(tau_flag, cutoff_h),   
                 aes(yintercept = cutoff_h), colour = "red") +
      geom_vline(xintercept = cutoff_v, colour = "red") +
      geom_text(data = text_flag_d, aes(label = case), hjust = 0, vjust = 0) +
      facet_wrap(~ tau_flag, scales = 'free_y') +
      xlab("Robust Distance") +
      ylab("|Residuals|")
```

* Generalized cook distance and Q function distance

We apply generalized cook distance and Q function distance methods in function `frame_mle`. This function returns generalized cook or q function distance for regression model on each given quantile. The results are also in tidy data structure which can be easily used for plotting the two distances with outliers marked.


```{r GCD,warning=FALSE,message=FALSE, results = "hide", fig.height=3, fig.width=8, fig.align = "center", fig.cap="Generalized cook distance of each observation on quantile 0.1, 0.5 and 0.9. Case 75 has relative large cook distance-funtion distance to other points"}
y <- ais_female$BMI
x <- cbind(1, ais_female$LBM, ais_female$Bfat)
case <- rep(1:length(y), length(tau))
GCD <- frame_mle(y, x, tau, error = 1e-06, iter = 10000,
                  method = 'cook.distance')
GCD_m <- cbind(case, GCD)
ggplot(GCD_m, aes(x = case, y = value )) +
    geom_point() +
    facet_wrap(~variable, scale = 'free_y') +
    geom_text(data = subset(GCD_m, value > mean(value) + 2*sd(value)),
              aes(label = case), hjust = 0, vjust = 0) +
    xlab("case number") +
    ylab("Generalized Cook Distance")
```

The same, visualization of Q function diagnostic results are shown in fig,

```{r QD,warning=FALSE,message=FALSE, results = "hide", fig.height=3, fig.width=8, fig.align = "center", fig.cap="Q function distance of each observation on quantile 0.1, 0.5 and 0.9. Case 75 has relative large Q function distance to other points"}

QD <- frame_mle(y, x, tau, error = 1e-06, iter = 10000,
               method = 'qfunction')
QD_m <- cbind(case, QD)
ggplot(QD_m, aes(x = case, y = value)) +
 geom_point() +
 facet_wrap(~variable, scale = 'free_y')+
 geom_text(data = subset(QD_m, value > mean(value) + sd(value)),
           aes(label = case), hjust = 0, vjust = 0) +
 xlab('case number') +
 ylab('Qfunction Distance')
```

Same as above, we also applied mean post probability, KL divergence to diagnose,

```{r BP,warning=FALSE,message=FALSE, fig.height=3, fig.width=8, fig.align = "center", fig.cap="Mean posterior probability of each case on quantile 0.1, 0.5 and 0.9. The mean posterior probabilities are calculated based on the postierior distribution of latent variable using Bayesian quantile regression method"}

y <- ais_female$BMI
x <- matrix(c(ais_female$LBM, ais_female$Bfat), ncol = 2, byrow = FALSE)
tau <- c(0.1, 0.5, 0.9)
case <- rep(1:length(y), length(tau))
prob <- frame_bayes(y, x, tau, M =  100,
                 method = 'bayes.prob')

prob_m <- cbind(case, prob)
ggplot(prob_m, aes(x = case, y = value )) +
   geom_point() +
   facet_wrap(~variable, scale = 'free') +
  geom_text(data = subset(prob_m, value > mean(value) + 2*sd(value)),
            aes(label = case), hjust = 0, vjust = 0) +
   xlab("case number") +
   ylab("Mean probability of posterior distribution")

```

```{r BKL,warning=FALSE,message=FALSE, fig.height = 3, fig.width = 8, fig.align = "center", fig.cap = "Kullback and Leibler divergence of each case on quantile 0.1, 0.5 and 0.9. The Kullback-Leibler divergence is calculated based on the postierior distribution of latent variable using Bayesian quantile regression method."}

kl <- frame_bayes(y, x, tau, M = 100,
                  method = 'bayes.kl')
kl_m <- cbind(case, kl)
ggplot(kl_m, aes(x = case, y = value)) +
  geom_point() +
  facet_wrap(~variable, scale = 'free')+
  geom_text(data = subset(kl_m, value > mean(value) + 2*sd(value)),
            aes(label = case), hjust = 0, vjust = 0) +
  xlab('case number') +
  ylab('Kullback-Leibler')
```

# Visualizing quantile regression

Visualization of quantile regression will help us understand the questions 'How does the shape of the model compare to the shape of the data?'. In addition, we can have a good impression of the location of models on each quantile. We use GGobi to visualize quantile regression model.

## Linear quantile regression model

In two predictors case, quantile regression models are lines in space. We use ais data fitting models and visualize them with GGobi.

```{r, out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model-single-pixel/linear-3D-1.png",
                          "Figures/QR-model-single-pixel/linear-3D-2.png",
                          "Figures/QR-model-single-pixel/linear-3D-3.png",
                          "Figures/QR-model-single-pixel/linear-3D-4.png",
                          "Figures/QR-model-single-pixel/linear-3D-5.png",
                          "Figures/QR-model-single-pixel/linear-3D-6.png",
                          "Figures/QR-model-single-pixel/linear-3D-7.png",
                          "Figures/QR-model-single-pixel/linear-3D-8.png",
                          "Figures/QR-model-single-pixel/linear-3D-9.png"))
```

In three predictor case, quantile regression models are cuboids in space which were displayed as follows,

```{r, out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model-single-pixel/linear-4D-1.png",
                          "Figures/QR-model-single-pixel/linear-4D-2.png",
                          "Figures/QR-model-single-pixel/linear-4D-3.png",
                          "Figures/QR-model-single-pixel/linear-4D-4.png",
                          "Figures/QR-model-single-pixel/linear-4D-5.png",
                          "Figures/QR-model-single-pixel/linear-4D-6.png",
                          "Figures/QR-model-single-pixel/linear-4D-7.png",
                          "Figures/QR-model-single-pixel/linear-4D-8.png",
                          "Figures/QR-model-single-pixel/linear-4D-9.png"))
```

## Non-linear quantile regression model

In non-linear case, we use elliptic hyperboloid and hyperbolic paraboloid as examples.

```{r, out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model/curve1-1.png",
                          "Figures/QR-model/curve1-2.png",
                          "Figures/QR-model/curve1-3.png",
                          "Figures/QR-model/curve1-4.png",
                          "Figures/QR-model/curve1-5.png",
                          "Figures/QR-model/curve1-6.png",
                          "Figures/QR-model/curve1-7.png",
                          "Figures/QR-model/curve1-8.png",
                          "Figures/QR-model/curve1-9.png"))
```

```{r, out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model/curve2-1.png",
                          "Figures/QR-model/curve2-2.png",
                          "Figures/QR-model/curve2-3.png",
                          "Figures/QR-model/curve2-4.png",
                          "Figures/QR-model/curve2-5.png",
                          "Figures/QR-model/curve2-6.png",
                          "Figures/QR-model/curve2-7.png",
                          "Figures/QR-model/curve2-8.png",
                          "Figures/QR-model/curve2-9.png"))
```

# Future work

high-dimensional and extreme quantile work.
