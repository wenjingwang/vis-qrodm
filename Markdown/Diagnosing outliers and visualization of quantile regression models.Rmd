---
title: "Diagnosing outliers and visualization of quantile regression models"
author:
- Wenjing Wang^1^, Dianne Cook^2^, Earo Wang^2^
- ^1^Renmin University of China  , ^2^Monash University
output:
  bookdown::pdf_document2: default
  fig_caption: yes
papersize: a4
fontsize: 11pt
---
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

```{r, message = FALSE, include = FALSE}
library(quokar)
library(quantreg)
library(tidyverse)
library(gridExtra)
library(purrr)
```


# Introduction

## Background of quantile regression

Quantile regression model has been widely used in many research areas such as economy, finance, social science (Autor, Houseman and Kerr (2017), Mitchell, Dowda and Pate (2017), Gallego-√Ålvarez and Ortas (2017), Korobilis (2017), Maciejowska, Nowotarski and Weron (2016)). Quantile regression has significant advantages over mean regression on two aspects:(a) observed covariates can describe the whole distribution of response variable instead of mean. (b) mean regression can no longer maintain the optimal properties due to heteroscedasticity or heavy tail distribution while quantile regression can deal with that.

Koenker and Bassett (1978) proposed linear model for the $\tau$th quantile is,

\begin{equation}
y_i=x^{'}_{i}\beta_{\tau}+\epsilon_{i}, \quad i=1,...,n
\label{eq:linear_qr}
\end{equation}

The $\tau$th quantile function of the sample is $Q_{y}(\tau|x)=x^{'}\beta(\tau)$. Based on the idea of minim izing a sum of asymmetrically weighted absolute residuals, the objective function of quantile regression model is,

\begin{equation}
\min_{\beta_{\tau} \in \mathbb{R}^{p}}\sum_{i=1}^{n} \rho_{\tau}(y_i-x_{i}^{'}\beta_{\tau})
\label{eq:object_function}
\end{equation}

where $\rho(.)$ is loss function which was defined as $\rho_{\tau}(u)=u(\tau-I(u <0))$. In addition, assuming $Y_1,...,Y_n$ is a sequence of i.i.d random variables which has distribution function $F$ and continuous density function $f$. The coefficience vector $\hat{\beta}_{\tau}$ is asymptotically normal, which can be expressed as,

\begin{equation}
\sqrt{n}(\hat{\beta}_{\tau}-\beta_{\tau}) \xrightarrow{d}
N(0,\tau(1-\tau)D^{-1}\varOmega_{x}D^{-1})
\label{eq:distrbution}
\end{equation}

where $D=E(f(\textbf{X}\beta)\textbf{X}\textbf{X}^{'})$ and $\varOmega_{x}=E(\textbf{X}^{'}\textbf{X})$.

Many current research focus on ebbeding quantile in other models to enhance model features or conduct better results analysis. Geraci (2014) proposed linear quantile mixed model which dealt with within-subject dependence by embeding subject-specific random intercepts into quantile regression model. Chernozhukov and Hansen (2006) used instrumental variable quantile regression for heterogeneous treatment effect models and simulaneous equations models to evaluate the impact of endogenous variables or treatments on the entire distribution of outcomes.

Another development is fitting quantile regression for specified data. Koenker (2004), Geraci and Bottai (2006) proposed to conduct quantile regression for longitudinal data. Parente and Santos Silva (2016) studied properties of the quantile regression estimator when data are sampled from independent and identically distributed clusters. Canay (2011), Arellano and Bonhomme (2016) introduce a class of quantile regression estimators for short panels. Galvao and Kato (2016),  presented fixed effects estimation of quantile regression models for panel data. 

Extensive theories has been used for inferencing quantile regression parameters. Gutenbrunner, Jureckova, Koenker, and Portnoy (1993) proposed rank-based inference to deal problems of constructing confidence intervals for individual quantile regression parameter estimates. Hahn (1995), Horowitz (1998), Fitzenberger (1998), He and Hu (1999) contributed to a variety of resampling methods to quantifying the robustness of inferencing. Koneker and Machado (1999) discussed inference for quantile regression based on Kolmogorov-Smironov goodness of fit method. To deal with "the Durbin problem", Koenker and Xiao (2002) developed new tests of the location shift and location-scale shift models for quantile regression process. There are also studies of quantile regression in bayesian framework which extended research ideas. 

Based on above numerous of methodology and application studies of quantile regression, extensive toolboxs conduct model fitting and inference has been developed. Free software R offers several packages implementing quantile regression, most famous `quantreg` by Roger Koenker, but also `gbm`, `quantregForest`, `qrnn`, `ALDqr` and `bayesQR`. However, few model diagnostic methods were proposed for quantile regression and no toolbox for model diagnostic were implemented in R. Statistical software SAS offered simple outlier diagnostic methods in procedure `QUANTREG`. Koenker(2017) also pointed out that more work needs to be done to develop better diagnostic tools for quantile regression models.

## Outlier detection in regression and HLMdiag

Sample data from the real world may have special points located far away from others. In regression model, these points may affect model fitting. In single variabe case, we can observe outliers based on scatter plot. Difficulty lies in high-dimensional situation, where statistical methods should be used.

Outliers in regression can be divided into two class, one is outliers in vertical direction and the other is leverage point. Various methods for detecting outliers have been studied(Atkinson 1994; Barnett and Lewis 1994; Becker and Gather 1999, 2001; Davies and Gather 1993; Gather and Becker 1997; Gnanadesikan and Kettenking 1972; Hadi 1992, 1994; Hawkins 1980; Maronna and Yohai 1995; Penny 1995; Rocke and Woodruff 1996; Rousseeuw and Van Zomeren 1990). Commonly used methods in mean regression including residuals, leverage value, studentized residuals and jacknife residuals.

Classic least ordinary square estimation of linear regresssion can be expressed as $\hat{\beta}=(X^{'}X)^{-1}X^{'}Y$, $\hat{Y}=X(X^{'}X)^{-1}X^{'}Y=HY$, where, $H$ is called hat matrix. Residuals can be write as $\hat{\epsilon}=Y-\hat{Y}(1-H)Y=(1-H)\epsilon$. The variance of the error term and the estimation of $Y$ are  $Var(\hat{\epsilon})=(1-H)$, $Var(\hat{Y})=X\hat{\beta}=H\sigma^{2}$. 

If taking outliers in $y$ and leverage points all in consideration, we can construct studentized residuals, which is $r_i=\frac{\hat{\epsilon}_{i}}{\sigma^{2}\sqrt{1-h_i}}$. The larger $r_i$, the more suspicious the outlier is.Another widely used outlier diagnositc idea is `leave-one-out`. Jackknif residual, knowing as $t_i=\frac{\hat{y}_{(i)}-\hat{y_i}}{\hat{\sigma}_{(i)}(1+x^{'}_{i}(X^{'}_{(i)}X_{(i)})^{-1}x_{i})^{-1/2}}$ and cook distance used this idea.


# Outlier detection in quantile regression

Quantile is more robust than mean when extreme values exist in the dataset interested. This property applies equally in regression context. Onyedikachi(2015) discussed the robustness of quantile regression using influence function. They used influence funtion to discuss the robustness of quantile and quantile regression.   

Set $T$ as a functional of $F$, the influence function is the directional derivative of $T(F)$ at $F$, and it measures the effect of a small perturbation in $F$ on $T(F)$. For Mean, the influence function is,

\begin{equation}
IF(y;T;F)=y-T(F)
\label{eq:mean}
\end{equation}

Suppose the estimator for the $\tau$th quantile is $\theta$. We have,

\begin{equation}
\theta = F^{-1}(\tau)
\label{eq: theta}
\end{equation}

For the $\tau$th quantile points, influence function can be expressed as,

\begin{equation}
IF(y;T;F)=\left\{
\begin{aligned}
\frac{\tau}{f(F^{-1}(\tau))} & ; & y > F^{-1}(\tau) \\
\frac{(\tau-1)}{f(F^{-1}(\tau))} & ; & y \leq F^{-1}(\tau) 
\end{aligned}
\right.
\label{eq:quantile_influence}
\end{equation}

where $f$ is the density function of $F$. Comparing \@ref(eq:mean) and \@ref(eq:quantile_influence), the latter obviously has boundary when $y$ is changing. We visualized the influence function by an example. The distribution function of y is $F(x)=1-e^{-\lambda x} \quad x>0$, $f(x)=e^{-x}$, $Q(tau)=-ln(1-p)$

```{r, eval = TRUE, echo = FALSE, fig.align="center", fig.height = 3, fig.width = 12, fig.cap = "Visualization of influence function for Mean and Quantile. It is obviously that quantile influence functions are bounded which indicating their robustness."}

F <- function(x){
  1-exp(-x)
}
f <- function(x){
  exp(-x)
}
Q <- function(p){
  -log(1-p)
}
x <- seq(0, 10, by = 0.001)
y <- sort(1-exp(-x))
inf_mean <- y - 0.01
data_m <- data.frame(y, inf_mean)
p <- ggplot(data_m, aes(y, inf_mean))+
  geom_line() +
  xlab("y") +
  ylab("Influence function of Mean")

inf_quantile <- function(y, tau){
  n <- length(y)
  n1 <- sum(y > Q(tau))
  n2 <- sum(y <= Q(tau))
  inf_q <- rep(tau/f(Q(tau)), n)
  inf_q[1:n2] <- rep((tau-1)/f(Q(tau)), n2)
  return(inf_q)
}

tau <- 0.1
data_q <- data.frame(inf_q = inf_quantile(y, 0.1), y = y)
p1 <- ggplot(data_q, aes(y, inf_q))+
  geom_line() + 
  geom_point(data = data.frame(a = Q(tau), b = -1), aes(a, b), shape = 1) +
  geom_point(data = data.frame(a = Q(tau), b = 0.1111), aes(a, b), shape = 16) +
  geom_vline(xintercept = Q(tau), colour = "red", linetype = "longdash") +
  geom_hline(yintercept = 0, colour = "red", linetype = "longdash") +
  xlab("y") +
  ylab("Influence function for quantile (tau = 0.1)")

tau <- 0.5
data_q <- data.frame(inf_q = inf_quantile(y, 0.5), y = y)
p2 <- ggplot(data_q, aes(y, inf_q))+
  geom_line() + 
  geom_point(data = data.frame(a = Q(tau), b = -1), aes(a, b), shape = 1) +
  geom_point(data = data.frame(a = Q(tau), b = 1), aes(a, b), shape = 16) +
  geom_hline(yintercept = 0, colour = "red", linetype = "longdash") +
  geom_vline(xintercept = max(y), colour = "red", linetype = "longdash") +
  xlab("y") +
  ylab("Influence function for quantile (tau = 0.5)")

tau <- 0.9
data_q <- data.frame(inf_q = inf_quantile(y, 0.9), y = y)
p3 <- ggplot(data_q, aes(y, inf_q))+
  geom_line() + 
  geom_point(data = data.frame(a = max(y), b = -1), aes(a, b), shape = 1) +
  geom_hline(yintercept = 0, colour = "red", linetype = "longdash") +
  geom_vline(xintercept = max(y), colour = "red", linetype = "longdash") +
  xlab("y") +
  ylab("Influence function for quantile (tau = 0.9)")
grid.arrange(p, p1, p2, p3, nrow = 1)
```

Onyedikachi (2015) also provided the influence function for quantile regression. Suppose $F$ represent the joint distribution of the pairs $(x,y)$, we have,

\begin{equation}
IF((y,x),\hat{\beta}_{F(\tau)},F)=Q^{-1}xsgn(y-x^{'}\hat{\beta}_{F}(\tau))
\label{eq:quantile_regression_influence}
\end{equation}

where 

\begin{equation}
dF=dG(x)f(y|x)dy
\label{eq: dg}
\end{equation}

\begin{equation}
Q=\int xx^{'}f(X^{'}\hat{\beta}_{F}(\tau))dG(x)
\label{eq: q_influence}
\end{equation}

\@ref(eq:quantile_regression_influence) implies that quantile regression estimates will not be affected by any change in the value of the dependent variable for some observations as long as the relative positions of the observation points to the fitted plane are maintained. 

## Displaying how do the outliers affect quantile regression

We conducted simulation to display how does outliers affect quantile regression estimations. In two simple simulation studies, we generate 100 sample observation and 3 outliers. The outliers are distributed in two locations in each case. We fitted mean regression and quantile regression based on these dataset to observe how do outliers affect model coefficients. 


```{r, eval=TRUE, echo=FALSE, fig.align="center", fig.height = 3, fig.width = 8,fig.align = "center", fig.cap = "Fitting quantile regression model on quantile 0.1, 0.5 and 0.9 using simulated datasets with and without outliers. The outliers located at the top-left of the original dataset. Results show that outliers pull up the slope of the 0.9 and 0.1 regression line. When outliers located at the bottom-right of the original dataset, results show that outliers pull down the slope of the 0.1 regression line."}

x <- sort(runif(100))
y1 <- 40*x + x*rnorm(100, 0, 10)
df <- data.frame(y1, x)
add_outlier <- data.frame(y1 = c(60,61,62), x = c(0.71, 0.73,0.75))
df_o <- rbind(df, add_outlier)

coef1 <- rq(y1 ~ x, tau = c(0.1, 0.5, 0.9), data = df, method = "br")$coef
rq_coef1 <- data.frame(intercept = coef1[1, ], coef = coef1[2, ],
                       tau_flag =colnames(coef1))

coef2 <- rq(y1 ~ x, tau = c(0.1, 0.5, 0.9),data = df_o, method = "br")$coef
rq_coef2 <- data.frame(intercept = coef2[1, ], coef = coef2[2, ],
                       tau_flag =colnames(coef2))
p1 <- ggplot(df_o) +
        geom_point(aes(x = x, y = y1), alpha = 0.1) +
        geom_abline(data = rq_coef1, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))+
        geom_abline(data = rq_coef2, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))

#####
x <- sort(runif(100))
y2 <- 40*x + x*rnorm(100, 0, 10)
df <- data.frame(y2, x)
add_outlier <- data.frame(y2 = c(1,2,3), x = c(0.71, 0.73,0.75))
df_o <- rbind(df, add_outlier)

coef1 <- rq(y2 ~ x, tau = c(0.1, 0.5, 0.9), data = df, method = "br")$coef
rq_coef1 <- data.frame(intercept = coef1[1, ], coef = coef1[2, ], tau_flag = colnames(coef1))

coef2 <- rq(y2 ~ x, tau = c(0.1, 0.5, 0.9), data = df_o, method = "br")$coef
rq_coef2 <- data.frame(intercept = coef2[1, ], coef = coef2[2, ], tau_flag = colnames(coef2))

p2 <- ggplot(df_o) +
        geom_point(aes(x = x, y = y2), alpha = 0.1) +
        geom_abline(data = rq_coef1, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))+
        geom_abline(data = rq_coef2, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))

grid.arrange(p1, p2, nrow = 1)
```

We also change the location of outliers or add outliers numbers to observe how they affect coeficient estimations on each quantile. These simulation studies are extended to multi-variables model.Using the simulated data to construct quantile regression model. By comparing the four models, we have a brief idea of the effect of outliers locating. The results show that when outliers moving down in y direction for 10 unit, it pulls down the slope on every quantile(comparing the result of model rq(y1~x) and rq(y2~x)). However, keeping moving down the outliers does no change to the slopes. This reflect the boundary theory expressed by influence function above.

```{r move-y1, warning=FALSE,message=FALSE, eval = TRUE, echo = FALSE, fig.height = 4, fig.width = 10, fig.align = "center", fig.cap = "Left fig: Fitting quantile regression models using simulated data. We keep moving down the outliers in y direction in y2 (y-5), y3 (y-10) and y4 (y-15). Right fig: Fitting quantile regression models using simulated data. We keep moving down the outliers in y direction getting datasets with variable y2 (=y-5), y3 (=y-10) and y4 (=y-15). Results show that in single predictor case, outliers moving down in y make no difference to the quantile regression coefficients estimations"}

x <- sort(runif(100))
y <- 40*x + x*rnorm(100, 0, 10)
selectedX <- sample(50:100,5)
y2<- y
y2[selectedX] <- x[1:5]*rnorm(5, 0, 10)
y3 <- y2
y3[selectedX] <- y2[selectedX] - 10
y4 <- y3
y4[selectedX] <- y3[selectedX] - 10
df <- data.frame(x, y, y2, y3, y4)
df_m <- df %>% gather(variable, value, -x)
p1 <- ggplot(df_m, aes(x = x, y=value)) +
        geom_point(alpha = 0.5) +
        xlab("x") +
        ylab("y") +
        facet_wrap(~variable, ncol=2, scale = "free_y") +
        geom_quantile(quantiles = seq(0.1, 0.9, 0.1), colour = "purple") +
        geom_smooth(method = "lm", se = FALSE, colour = "orange")

coefs <- 2:5 %>%
  map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
  map_df(~ as.data.frame(t(as.matrix(coef(.)))))
colnames(coefs) <- c("intercept", "slope")
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")

df_m1 <- data.frame(model, tau, coefs)
df_mf <- df_m1 %>% gather(variable, value, -c(model, tau))
p2 <- ggplot(df_mf, aes(x = tau, y = value, colour = model)) +
        geom_point() +
        geom_line() +
        facet_wrap(~ variable, scale = "free_y") +
        xlab('quantiles') +
        ylab('coefficients')

grid.arrange(p1, p2, nrow = 1)
```

We also observed the change of coefficients in multi-variable model. The results show that coefficients changes slowly when keep moving down the outliers in y-direction.

```{r move-y-multi1, eval=TRUE, echo=FALSE,warning=FALSE,message=FALSE, fig.height = 3, fig.width = 8, fig.align = "center", fig.cap = "Fitting quantile regression models using simulated data. We keep moving down the outliers in y direction getting three datasets with different locations of outliers (changing in y-aixs, y2 (=y-5), y3 (=y-10) and y4 (=y-15)). Results show that in multi predictors case, outliers moving down in y make small change to the quantile regression coefficients estimations"}

n <- 100
set.seed(101)
x1 <- sort(rnorm(n, 0, 1))
x2 <- sort(rnorm(n, 1, 2))
y <- 40*(x1 + x2) + x1*rnorm(100, 0, 10) + x2*rnorm(100, 0, 10)
selectedX <- sample(50:100,5)
y2<- y
y2[selectedX] <- x1[1:5]*rnorm(5, 0, 10) + x2[1:5]*rnorm(5, 0, 10)
y3 <- y2
y3[selectedX] <- y2[selectedX] - 100
y4 <- y3
y4[selectedX] <- y3[selectedX] - 100
df <- data.frame(y, y2, y3, y4, x1, x2)
coefs <- 1:4 %>%
  map(~ rq(df[, .] ~ x1 + x2, data = df, seq(0.1, 0.9, 0.1))) %>%
  map_df(~ as.data.frame(t(as.matrix(coef(.)))))
colnames(coefs) <- c("intercept", "slope_x1", "slope_x2")
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(model, tau, coefs)
df_mf <- df_m1 %>% gather(variable, value, -c(model, tau))
ggplot(df_mf, aes(x = tau, y = value, colour = model)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ variable, scale = "free_y") +
  xlab('quantiles') +
  ylab('coefficients')

```

If moving outliers in same pattern moving on x direction, slopes change every time outlier moves. To go further, each move does different effect on different quantiles.

```{r move-x1, eval=TRUE, echo=FALSE, warning=FALSE,message=FALSE, fig.height = 4, fig.width = 10, fig.align = "center", fig.cap = "Left fig: Fitting quantile regression models using simulated data. We keep moving the outliers to the right in x direction getting three datasets with different locations of outliers (changing in x-aixs, x2 (=x+0.2), x3 (=x+0.4) and x4 (=x+0.6)). Right fig: Fitting quantile regression models using simulated data. We keep moving the outliers to the right in x direction getting three datasets with different locations of outliers (changing in x-aixs, x2 (=x+0.2), x3 (=x+0.4) and x4 (=x+0.6)).Results show that in single predictors case, outliers moving right in x make significant change to the quantile regression coefficients estimations."}

x <- sort(runif(100))
y <- 40*x + x*rnorm(100, 0, 10)
selectedIdx <- sample(50:100,5)
df <- data.frame(y)
df$y2 <- y
df$x <- x
df$y2[selectedIdx] <- df$x[1:5]*rnorm(5, 0, 10)
df$x2 <- x
df$x2[selectedIdx] <- df$x2[selectedIdx] + 0.2
df$x3 <- df$x2
df$x3[selectedIdx] <- df$x3[selectedIdx] + 0.2
df$x4 <- df$x3
df$x4[selectedIdx] <- df$x4[selectedIdx] + 0.2
df_m <- df %>% gather(variable, value, -y, -y2)
p1 <- ggplot(df_m, aes(x = value, y=y2)) +
  geom_point() +
  xlab("x") +
  ylab("y") +
  facet_wrap(~variable, ncol=2, scale = "free") +
  geom_quantile(quantiles = seq(0.1, 0.9, 0.1))

coefs <- 3:6 %>%
  map(~ rq(df$y2 ~ df[, .], data = df, seq(0.1, 0.9, 0.1))) %>%
  map_df(~ as.data.frame(t(as.matrix(coef(.)))))
colnames(coefs) <- c("intercept", "slope")
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(model, tau, coefs)
df_mf <- df_m1 %>% gather(variable, value, -c(model, tau))
p2 <- ggplot(df_mf, aes(x = tau, y = value, colour = model)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ variable, scale = "free_y") +
  xlab('quantiles') +
  ylab('coefficients')

grid.arrange(p1, p2, nrow = 1)
```

In conclusion, quantile regression response differently to outliers comparing mean regression in two aspects: (a) not all models on each quantile will be affected when outliers exist. If we are interested in model on particular quantile, the effect of outliers should be carefully considered. (b) quantile regression model do not have robustness properties to so called leverage points. 

# Outlier diagnosting methods for quantile regression 

We proposed three methods for quantile regression outlier diagnostic which will be discussed as follows.

* Standard residual-Robust Distance

We can not use the famous "Hat Matrix" to detect leverage points in quantile regression since the coefficient estimation of quantile regression do not satisfy $\hat{\beta}=(X^{'}X)^{-1}X^{'}Y$. One way to identify possible leverage points is to calculate a distance from each point to a "center" of the data. Leverage point would then be the one with a distance larger than some predetermined cutoff. A conventional measurement is Mahalanobi distance:

\begin{equation}
MD(x_i) = [(x_i-\bar{x})^{'}\bar{\textbf{C}}(\textbf{A})^{-1}(x_i-\bar{x})]^{1/2}
\label{eq:distance}
\end{equation}

where $\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i$ and $\bar{\textbf{C}}(\textbf{A})=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^{'}(x_i-\bar{x})$ are the empirical multivariate location and scale respectively. However, the standard sample location and scale parameters are not robust to outliers. In addition, datasets with multiple outliers or clusters of outliers are subject to problems of masking and swamping (Pearson and Chandra Sekar 1936). Such problems of unrobust, masking and swamping can be resolved by using robust estimates of shape and location, which by definition are less affected by outliers (Rousseeuw and van Zomeren (1991)). We use Rousseeuw's minimum covariance determinant (MCD) proposed by Rousseeuw and Van Driessen (1999) to estimate the location and scale of the data.

The MCD estimator can be defined as:

\begin{equation}
MCD = (\bar{X}^{*}_{h}, S^{*}_{h})
\label{eq: mcd}
\end{equation}

where $X$ and $S$ stand for location and scale. $h={p: |S^{*}_{h}|<|S^{*}_{k}|,|k|=p}$, $\bar{X}^{*}_{h}=\frac{1}{p}\sum_{i \in p}x_{i}$, $S^{*}_{p}=\frac{1}{p}\sum_{i \in p}(x_i-\bar{X}^{*}_{p})(x_i-\bar{X}^{*}_{p})^{'}$.

The value $p$ can be thought of as the minimum number of points which must not be outliers. The MCD has its highest possible breakdown at $h=[\frac{n+p+1}{2}]$ where $[.]$ is the greatest integer function. Because we are interested in outlier detection, we will use $h$ at its highest possible breakdown. $h=[\frac{n+p+1}{2}]$ in our calculations, and we refer to a sample of size $h$ as a "half sample" The MCD is omputed from the "closet" half sample, and therefore, the outlying points will have little affect on the MCD location or shape estimate. With MCD, we can calculate robust distance which was defined as,

\begin{equation}
RD(x_i)=[(x_i-\textbf{T(A)})^{'}\textbf{C(A)}^{-1}(x_i-\textbf{T(A)})]^{1/2}
\label{eq:rd}
\end{equation}

Where $\textbf{T(A)}$ and $\textbf{C(A)}$ are robust multivariate location and scale estimates that are computed according to the MCD.

Package `quokar` provide Mahalanobi distance and Robust distance to detect leverage points in quantile regression. Residuals that are based on quantile regression estimates are used to detect vertical outliers.
 
* Cook's Distance and Likelihood Distance

Case-deletion is another classical approach to study the outlier effects in regression by leaving 1 or k observations out every time fitting the regression model. case-deletion diagnostics such as Cook's distance or the likelihoood distance have been successfully applied to various statistical models. Based on the research of S√°nchez, Lachos and Labra (2013), we applied generalized cook's distance and likelihood distance for quantile regression outlier diagnostic in package `quokar`.

Yu and Moyeed (2001) proposed random variable $Y$ distributed as asymmetric Laplace distribution with location parameter $\mu$, scale parameter $\sigma >0$ and skewness parameter $\tau \in (0,1)$ has density function:

\begin{equation}
f(y|\mu, \sigma, \tau) = \frac{\tau (1-\tau)}{\sigma}exp{-\rho_{p}(\frac{(y-\mu)}{\sigma})}
\label{eq: ald}
\end{equation}

where $\rho_{\tau}(.)$ is the loss function mentioned above. 

Suppose that $y_i \sim ALD(\textbf{x}^{'}_{i}\mathbf{\beta}_{p}, \sigma, \tau)$, $i=1,...,n$ are independent. The likelihood function for $n$ observations is

\begin{equation}
L(\mathbf{\beta},\sigma|y)=\frac{\tau^{n}(1-\tau)^{n}}{\sigma^{n}}exp{-\sum_{i=1}^{n} \rho_{\tau}(\frac{y_i-\textbf{x}^{'}_{i}}{\sigma})}
\label{eq: ald_likelihood}
\end{equation}

For note, a quantity with a subscript '[i]' means the relevant quantity with the $i$th observation deleted. Let $\hat{\theta}$ and $\hat{\theta}^{*}_{[i]}$ be the maximum likelihood estimator of $theta$ based on $L(\theta|Y)$ and $L(\theta|Y_{[i]})$ respectively. Cook's distance $CD_{i}$ is given by \@ref(cd). For external norms, $M$ is usually chosen to be $-\ddot{L(Y|\theta)}$.

\begin{equation}
CD_{i}=(\hat{\theta}^{1}_{[i]}-\hat{\theta})^{'}M(\hat{\theta}^{1}_{[i]}-\hat{\theta})
\label{eq:cd}
\end{equation}

Alternatively, another measure of difference between $\theta$ and $\theta^{*}_{[i]}$ is the observed data likelihood function which is defined as 
Likelihood distance.

\begin{equation}
LD_i=L(\hat{\theta}|Y)-L(\hat{\theta}^{1}_{[i]}|Y)
\label{eq: ld}
\end{equation}

The $i$th observation is regarded as influential if the value of Cook's distance or Likelihood distance is relatively large.

S√°nchez and Lachos (2015) proposed a EM algorithm to calculate the above Cook's distance and Likelihood distance for quantile regression using asymmetric laplace distribution. 

\begin{equation}
Q(\theta|\hat{\theta})=E{L(\theta|Y)|\hat{\theta}}
\label{eq:q_function}
\end{equation}

To assess the influence of the $i$th case, we will consider the function

\begin{equation}
Q_{[i]}(\theta|\hat{\theta})=E{L(\theta|Y_{[i]})|\hat{\theta}}
\label{eq: q_one_deletion}
\end{equation}

Let $\hat{\theta}_{[i]}$ be the maximiser of $Q_{[i]}(\theta|\hat{\theta})$. The one-step approximation $\hat{\theta}_{[i]}$ is

\begin{\equation}
\hat{\theta}_{[i]}=\hat{\theta}+\{-\ddot{Q}(\hat{\theta}|\hat{\theta})\}^{-1}\dot{Q}_{[i]}(\hat{\theta}|\hat{\theta})
\label{eq: estimator}
\end{equation}

where

$$\ddot{Q}(\hat{\theta}|\hat{\theta})=\frac{\partial^{2}Q(\theta|\hat{\theta})}{\partial\theta\partial \theta^{T}}|_{\theta=\hat{\theta}}$$

$$\dot{Q}_{[i]}(\hat{\theta}|\hat{\theta})=\frac{\partial Q_{[i]}(\theta|\hat{\theta})}{\partial\theta}|_{\theta=\hat{\theta}}$$


are the Hessian matrix and the gradient vector evaluated at $\hat{\theta}$, respectively.

For measuring the distance between $\hat{\theta}_{[i]}$ and $\hat{\theta}$. We consider generalized cook distance as follows.

$$GD_{i} =(\hat{\theta}_{[i]}-\hat{\theta})^{T}\{-Q(\hat{\theta}|\hat{\theta})\}(\hat{\theta}_{[i]}-\hat{\theta}), i=1,...,n$$

The measurement of the influence of the $i$th case is based on the Q-distance function, similar to the likelihood distance $LD_{i}$ which was defined as

$$QD_{i}=2\{Q(\hat{\theta}|\hat{\theta})-Q(\hat{\theta}_{[i]}|\hat{\theta})\}$$

* Mean Post Probability 

Baysian quantile regression added a latent variable $v_i$ into model for each observation. Every $v_i$ is assumed to have an exponential distribution with mean $\sigma$, that with the likelihood produces a posterior distributed according to a generalized inverse Gaussian with parameters.

If we define the variable $O_i$, which takes value equal to 1 when the $i$th observation is an outlier, and 0 otherwise. Then we propose to calculate the pro
bility of an observation being an outlier as

$$P(O_i=1)=\frac{1}{n-1}\sum_{j \neq i}P(v_i > v_j|data)$$

The probability in the expression above can be approximated given the
MCMC draws, as follows:

$$P(O_i = 1)=\frac{1}{M}I(v^{(l)_i}>max_{k \in 1:M}v^{(k)}_j)$$

where $M$ is the size of the chain of $v_i$ after the burn-in perior and $v^{(l)}_i$ is the $l$th draw of this chain.


* K-L Divergence

Similar with mean posterior probability method, we also caculates Kullback-Leibler divergence which proposed by Kullback and Leibler(1951) as a more precise method of measuring the distance between those latent variables in Bayes quantile regression. The Kullback-Leibler divergence is defined as:
  
$$K(f_i, f_j)=\int log(\frac{f_i(x)}{f_j(x)}f_{i}(x))dx$$
  
where $f_i$ could be the posterior conditional distribution of $v_i$ and $f_j$ the posterior conditional distribution of $v_j$. We should average this divergence for one observation based on the distance from all others,
  
$$KL(f_i)=\frac{1}{n-1}\sum_{j\neq i}K(f_i, f_j)$$
  
The outliers should show a high probability value for this divergence. We compute the integral using the trapezoidal rule.

# Examining outlier detection

We developed R package `quokar` to implete quantile regression outlier diagnostic methods. This package mainly realized two basic features: (a) plot the outlier states; (b) plot data with outliers marked. `quokar` is available from Github at https://github.com/wenjingwang/quokar, so to install and load withn R use:

```{r, eval = FALSE, echo = FALSE}
devtools::install_github("wenjingwang/quokar")
library(quokar)
```

We implete ais data as an example to introduce this package. AIS data include 14 variables for 100 female atheletes.

## Plot the outlier stats

In single variable case, we can use scatter plot to represent the outlier stats. The following code showed how to display suspicious outliers based on quantile regression models.

```{r, warning=FALSE,message=FALSE, fig.height=3, fig.width=5, fig.align = "center", fig.cap="Plot the outlier stats."}
data(ais)
ais_female <- filter(ais, Sex == 1)
case <- 1 : nrow(ais_female)
ais_female <- cbind(case, ais_female)
coef_rq <- coef(rq(BMI ~ LBM, tau = c(0.1, 0.5, 0.9),
                   data = ais_female, method = "br"))

br_coef <- data.frame(intercept = coef_rq[1, ],
                      coef = coef_rq[2, ],
                      tau_flag = colnames(coef_rq))
ggplot(ais_female)+
  geom_point(aes(x = LBM, y = BMI)) +
  geom_abline(data = br_coef, aes(intercept = intercept,
                                  slope = coef,
                                  colour = tau_flag), size = 1) +
  geom_text(data = subset(ais_female, case %in% c(1, 75)),
                          aes(x = LBM, y = BMI, label = case), 
            colour = "red",hjust = 0, vjust = 0) +
  scale_colour_brewer("Dark2") +
  theme_dark()

```

## Plot data with outliers marked

Scatter plot has limitations when tackling multi-variable regression cases. In `quokar`, we provide functions to do outlier diagnostic which return the dataframe easily to plot data with outliers marked.

* residual-robust distance method

First, we calculate residuals, mahananobi distance and robust distance for quantile regression using function `plot_distance`. Simutaneously, it provides the cutoff value for identifying the outliers in regression models.

```{r}
tau <- c(0.1, 0.5, 0.9)
object <- rq(BMI ~ LBM + Bfat, data = ais_female, tau = tau)
plot_distance <- frame_distance(object, tau = c(0.1, 0.5, 0.9))
distance <- plot_distance[[1]]
head(distance, 3)
cutoff_v <- plot_distance[[2]]; cutoff_v
cutoff_h <- plot_distance[[3]]; cutoff_h
```

Function `plot_distance` returns the tidy data form for plotting data with outliers marked and overlaying the cutoff lines. 

```{r, warning=FALSE,message=FALSE, fig.height=3, fig.width=8, fig.align = "center", fig.cap="Robust Distance-Residual Plot. Points on the right of vertical cutoff line are considered leverage points and points above the horizental cutoff line are outliers in y-direction."}

n <- nrow(object$model)
case <- rep(1:n, length(tau))
distance <- cbind(case, distance)
distance$residuals <- abs(distance$residuals)
tau_flag <- paste("tau", tau, sep="")
text_flag <- 1:length(cutoff_h) %>%
                            map(function(i){
                            distance %>% 
                            filter((residuals > cutoff_h[i] |rd > cutoff_v)
                                & tau_flag == tau_flag[i])})
##need polish
text_flag_d <- rbind(text_flag[[1]], text_flag[[2]], text_flag[[3]])
ggplot(distance, aes(x = rd, y = residuals)) +
      geom_point() +
      geom_hline(data = data.frame(tau_flag, cutoff_h),   
                 aes(yintercept = cutoff_h), colour = "red") +
      geom_vline(xintercept = cutoff_v, colour = "red") +
      geom_text(data = text_flag_d, aes(label = case), hjust = 0, vjust = 0) +
      facet_wrap(~ tau_flag, scales = 'free_y') +
      xlab("Robust Distance") +
      ylab("|Residuals|")
```

* Generalized cook distance and Q function distance

We apply generalized cook distance and Q function distance methods in function `frame_mle`. This function returns generalized cook or q function distance for regression model on each given quantile. The results are also in tidy data structure which can be easily used for plotting the two distances with outliers marked.


```{r,warning=FALSE,message=FALSE, results = "hide", fig.height=3, fig.width=8, fig.align = "center", fig.cap="Generalized cook distance of each observation on quantile 0.1, 0.5 and 0.9. Case 75 has relative large cook distance-funtion distance to other points"}
y <- ais_female$BMI
x <- cbind(1, ais_female$LBM, ais_female$Bfat)
case <- rep(1:length(y), length(tau))
GCD <- frame_mle(y, x, tau, error = 1e-06, iter = 10000,
                  method = 'cook.distance')
GCD_m <- cbind(case, GCD)
ggplot(GCD_m, aes(x = case, y = value )) +
    geom_point() +
    facet_wrap(~variable, scale = 'free_y') +
    geom_text(data = subset(GCD_m, value > mean(value) + 2*sd(value)),
              aes(label = case), hjust = 0, vjust = 0) +
    xlab("case number") +
    ylab("Generalized Cook Distance")
```

The same, visualization of Q function diagnostic results are shown in fig,

```{r, warning=FALSE,message=FALSE, results = "hide", fig.height=3, fig.width=8, fig.align = "center", fig.cap="Q function distance of each observation on quantile 0.1, 0.5 and 0.9. Case 75 has relative large Q function distance to other points"}

QD <- frame_mle(y, x, tau, error = 1e-06, iter = 10000,
               method = 'qfunction')
QD_m <- cbind(case, QD)
ggplot(QD_m, aes(x = case, y = value)) +
 geom_point() +
 facet_wrap(~variable, scale = 'free_y')+
 geom_text(data = subset(QD_m, value > mean(value) + sd(value)),
           aes(label = case), hjust = 0, vjust = 0) +
 xlab('case number') +
 ylab('Qfunction Distance')
```

Same as above, we also applied mean post probability, KL divergence to diagnose,

```{r, eval = FALSE, echo = FALSE, warning=FALSE,message=FALSE, fig.height=3, fig.width=8, fig.align = "center", fig.cap="Mean posterior probability of each case on quantile 0.1, 0.5 and 0.9. The mean posterior probabilities are calculated based on the postierior distribution of latent variable using Bayesian quantile regression method"}

y <- ais_female$BMI
x <- matrix(c(ais_female$LBM, ais_female$Bfat), ncol = 2, byrow = FALSE)
tau <- c(0.1, 0.5, 0.9)
case <- rep(1:length(y), length(tau))
prob <- frame_bayes(y, x, tau, M =  10, burn = 1, 
                 method = 'bayes.prob')

prob_m <- cbind(case, prob)
ggplot(prob_m, aes(x = case, y = value )) +
   geom_point() +
   facet_wrap(~variable, scale = 'free') +
  geom_text(data = subset(prob_m, value > mean(value) + 2*sd(value)),
            aes(label = case), hjust = 0, vjust = 0) +
   xlab("case number") +
   ylab("Mean probability of posterior distribution")

```

```{r,eval = FALSE, echo = FALSE, warning=FALSE,message=FALSE, fig.height = 3, fig.width = 8, fig.align = "center", fig.cap = "Kullback and Leibler divergence of each case on quantile 0.1, 0.5 and 0.9. The Kullback-Leibler divergence is calculated based on the postierior distribution of latent variable using Bayesian quantile regression method."}

kl <- frame_bayes(y, x, tau, M = 10, burn = 1,
                  method = 'bayes.kl')
kl_m <- cbind(case, kl)
ggplot(kl_m, aes(x = case, y = value)) +
  geom_point() +
  facet_wrap(~variable, scale = 'free')+
  geom_text(data = subset(kl_m, value > mean(value) + 2*sd(value)),
            aes(label = case), hjust = 0, vjust = 0) +
  xlab('case number') +
  ylab('Kullback-Leibler')
```

# Visualizing quantile regression

Visualization of quantile regression will help us understand the questions 'How does the shape of the model compare to the shape of the data?'. In addition, we can have a good impression of the location of models on each quantile. We use GGobi to visualize quantile regression model.

## Linear quantile regression model

In two predictors case, quantile regression models are lines in space. We use ais data fitting models and visualize them with GGobi.

```{r, eval=TRUE, echo=TRUE, fig.align="center",out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model-single-pixel/linear-3D-1.png",
                          "Figures/QR-model-single-pixel/linear-3D-2.png",
                          "Figures/QR-model-single-pixel/linear-3D-3.png",
                          "Figures/QR-model-single-pixel/linear-3D-4.png",
                          "Figures/QR-model-single-pixel/linear-3D-5.png",
                          "Figures/QR-model-single-pixel/linear-3D-6.png",
                          "Figures/QR-model-single-pixel/linear-3D-7.png",
                          "Figures/QR-model-single-pixel/linear-3D-8.png",
                          "Figures/QR-model-single-pixel/linear-3D-9.png"))
```

In three predictor case, quantile regression models are cuboids in space which were displayed as follows,

```{r, eval=TRUE, echo=TRUE, fig.align="center", out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model-single-pixel/linear-4D-1.png",
                          "Figures/QR-model-single-pixel/linear-4D-2.png",
                          "Figures/QR-model-single-pixel/linear-4D-3.png",
                          "Figures/QR-model-single-pixel/linear-4D-4.png",
                          "Figures/QR-model-single-pixel/linear-4D-5.png",
                          "Figures/QR-model-single-pixel/linear-4D-6.png",
                          "Figures/QR-model-single-pixel/linear-4D-7.png",
                          "Figures/QR-model-single-pixel/linear-4D-8.png",
                          "Figures/QR-model-single-pixel/linear-4D-9.png"))
```

## Non-linear quantile regression model

In non-linear case, we use elliptic hyperboloid and hyperbolic paraboloid as examples.

```{r, eval=TRUE, echo=TRUE, fig.align="center", out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model/curve1-1.png",
                          "Figures/QR-model/curve1-2.png",
                          "Figures/QR-model/curve1-3.png",
                          "Figures/QR-model/curve1-4.png",
                          "Figures/QR-model/curve1-5.png",
                          "Figures/QR-model/curve1-6.png",
                          "Figures/QR-model/curve1-7.png",
                          "Figures/QR-model/curve1-8.png",
                          "Figures/QR-model/curve1-9.png"))
```

```{r, include = FALSE, out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model/curve2-1.png",
                          "Figures/QR-model/curve2-2.png",
                          "Figures/QR-model/curve2-3.png",
                          "Figures/QR-model/curve2-4.png",
                          "Figures/QR-model/curve2-5.png",
                          "Figures/QR-model/curve2-6.png"))
```

# Future work

high-dimensional and extreme quantile work.

# Reference

Koenker R, Machado J A F. Goodness of fit and related inference processes for quantile regression[J]. Journal of the american statistical association, 1999, 94(448): 1296-1310.

Fitzenberger B. The moving blocks bootstrap and robust inference for linear least squares and quantile regressions[J]. Journal of Econometrics, 1998, 82(2): 235-287.

Chernozhukov V, Hansen C. Instrumental variable quantile regression: A robust inference approach[J]. Journal of Econometrics, 2008, 142(1): 379-398.

Geraci M, Bottai M. Quantile regression for longitudinal data using the asymmetric Laplace distribution[J]. Biostatistics, 2007, 8(1): 140-154.

Koenker R. Quantile regression for longitudinal data[J]. Journal of Multivariate Analysis, 2004, 91(1): 74-89.

Korobilis D. Quantile regression forecasts of inflation under model uncertainty[J]. International Journal of Forecasting, 2017, 33(1): 11-20.

Autor D H, Houseman S N, Kerr S P. The Effect of Work First Job Placements on the Distribution of Earnings: An Instrumental Variable Quantile Regression Approach[J]. Journal of Labor Economics, 2017, 35(1): 149-190.


Mitchell J A, Dowda M, Pate R R, et al. Physical Activity and Pediatric Obesity: A Quantile Regression Analysis[J]. Medicine and science in sports and exercise, 2017, 49(3): 466.

Gallego-√Ålvarez I, Ortas E. Corporate environmental sustainability reporting in the context of national cultures: A quantile regression approach[J]. International Business Review, 2017, 26(2): 337-353.

Maciejowska K, Nowotarski J, Weron R. Probabilistic forecasting of electricity spot prices using Factor Quantile Regression Averaging[J]. International Journal of Forecasting, 2016, 32(3): 957-965.


Parente P M D C, Santos Silva J. Quantile regression with clustered data[J]. Journal of Econometric Methods, 2016, 5(1): 1-15.

Galvao A F, Kato K. Smoothed quantile regression for panel data[J]. Journal of Econometrics, 2016, 193(1): 92-112.

Arellano M, Bonhomme S. Nonlinear panel data estimation via quantile regressions[J]. The Econometrics Journal, 2016, 19(3).

Canay I A. A simple approach to quantile regression for panel data[J]. The Econometrics Journal, 2011, 14(3): 368-386.

Geraci M. Linear quantile mixed models: the lqmm package for Laplace quantile regression[J]. Journal of Statistical Software, 2014, 57(13): 1-29.

Chernozhukov V, Hansen C. Instrumental quantile regression inference for structural and treatment effect models[J]. Journal of Econometrics, 2006, 132(2): 491-525.