---
title: "Diagnosing outliers and visualization of quantile regression models"
author:
- Wenjing Wang^1^, Dianne Cook^2^, Earo Wang^2^
- ^1^Renmin University of China  , ^2^Monash University
fontsize: 11pt
papersize: a4
output: 
  bookdown::pdf_document2:
    fig_caption: yes
---

```{r, message = FALSE, include = FALSE}
library(quokar)
library(quantreg)
library(tidyverse)
library(gridExtra)
library(purrr)
```


# Introduction

## Background of quantile regression

Quantile regression model has been widely used in many research areas such as economy, finance, social science and many other areas (Autor, Houseman and Kerr (2017), Mitchell, Dowda and Pate (2017), Gallego-√Ålvarez and Ortas (2017), Korobilis (2017), Maciejowska, Nowotarski and Weron (2016)). Observed covariates in quantile regression can describe the distribution of response variable which extend the mean regression analysis. In addition, mean regression can no longer maintain the optimal properties due to heteroscedasticity or heavy tail distribution. 

Linear quantile regression model can be present as $y=X^{'}\beta+\epsilon$, where $y$ is response variable, $X$ is covariate variable vector, $\beta$ and $\epsilon$ are coefficient vector and error term. The $\tau$th quantile function of the sample is $Q_{y}(\tau|x)=X^{'}\beta(\tau)$. Based on the idea of minimizing a sum of asymmetrically weighted absolute residuals, the objective function of quantile regression model is,

$$\min\sum_{i=1}^{n} \rho_{\tau}(y_i-X^{'}\beta)$$

where $\rho(.)$ is loss function which was defined as $\rho_{\tau}(u)=u(\tau-I(u <0))$

Assuming $Y_1,...,Y_n$ is a sequence of i.i.d random variables, which has distribution function $F$ and continuous density function $f$. The covariant matrix is $\textbf{X}$. Quantile regressoin coefficient vector $\hat{\beta}_{\tau}$ is asymptotically normal,

$$\sqrt{n}(\hat{\beta}_{\tau}-\beta_{\tau}) \xrightarrow{d} N(0, \tau(1-\tau)D^{-1}\varOmega_{x}D^{-1})$$

where

$D=E(f(\textbf{X}\beta)\textbf{XX^{'}})$ and $\varOmega_{x}=E(\textbf{X}^{'}\textbf{X})$

In current research trend, quantile regression has been embeded in other models to enhance model features or conduct better results analysis. Geraci (2014) proposed linear quantile mixed model which dealt with within-subject dependence by embeding subject-specific random intercepts into quantile regression model. Chernozhukov and Hansen (2006) used instrumental variable quantile regression for heterogeneous treatment effect models and simulaneous equations models to evaluate the impact of endogenous variables or treatments on the entire distribution of outcomes.

Another development is fitting quantile regression for specified data class. Koenker (2004), Geraci and Bottai (2006) proposed to conduct quantile regression for longitudinal data. Parente and Santos Silva (2016) studied properties of the quantile regression estimator when data are sampled from independent and identically distributed clusters. Canay (2011), Arellano and Bonhomme (2016) introduce a class of quantile regression estimators for short panels. Galvao and Kato (2016),  presented fixed effects estimation of quantile regression models for panel data. 

Extensive theories has been used for inferencing quantile regression parameters. Gutenbrunner, Jureckova, Koenker, and Portnoy (1993) proposed rank-based inference to deal problems of constructing confidence intervals for individual quantile regression parameter estimates. Hahn (1995), Horowitz (1998), Fitzenberger (1998), He and Hu (1999) contributed to a variety of resampling methods to quantifying the robustness of inferencing. Koneker and Machado (1999) discussed inference for quantile regression based on Kolmogorov-Smironov goodness of fit method. To deal with "the Durbin problem", Koenker and Xiao (2002) developed new tests of the location shift and location-scale shift models for quantile regression process. There are also studies of quantile regression in bayesian framework which extended research ideas. 

Based on above numerous of methodology and application studies of quantile regression, extensive toolboxs conduct model fitting and inference has been developed. Free software R offers several packages implementing quantile regression, most famous `quantreg` by Roger Koenker, but also `gbm`, `quantregForest`, `qrnn`, `ALDqr` and `bayesQR`. However, few model diagnostic methods were proposed for quantile regression and no toolbox for model diagnostic were implemented in R. Statistical software SAS offered simple outlier diagnostic methods in procedure `QUANTREG`. Koenker(2017) also pointed out that more work needs to be done to develop better diagnostic tools for quantile regression models.

## Outlier detection in regression and HLMdiag

Sample data from the real world may have special points located far away from others. In regression model, these points may affect model fitting. In single variabe case, we can observe outliers based on scatter plot. Difficulty lies in high-dimensional situation, where statistical methods should be used.

Outliers in regression can be divided into two class, one is outliers in vertical direction and the other is leverage point. Various methods for detecting outliers have been studied(Atkinson 1994; Barnett and Lewis 1994; Becker and Gather 1999, 2001; Davies and Gather 1993; Gather and Becker 1997; Gnanadesikan and Kettenking 1972; Hadi 1992, 1994; Hawkins 1980; Maronna and Yohai 1995; Penny 1995; Rocke and Woodruff 1996; Rousseeuw and Van Zomeren 1990). Commonly used methods in mean regression including residuals, leverage value, studentized residuals and jacknife residuals.

Classic least ordinary square estimation of linear regresssion can be expressed as $\hat{\beta}=(X^{'}X)^{-1}X^{'}Y$, $\hat{Y}=X(X^{'}X)^{-1}X^{'}Y=HY$, where, $H$ is called hat matrix. Residuals can be write as $\hat{\epsilon}=Y-\hat{Y}(1-H)Y=(1-H)\epsilon$. The variance of the error term and the estimation of $Y$ are  $Var(\hat{\epsilon})=(1-H)$, $Var(\hat{Y})=X\hat{\beta}=H\sigma^{2}$. 

If taking outliers in $y$ and leverage points all in consideration, we can construct studentized residuals, which is $r_i=\frac{\hat{\epsilon}_{i}}{\sigma^{2}\sqrt{1-h_i}}$. The larger $r_i$, the more suspicious the outlier is.Another widely used outlier diagnositc idea is `leave-one-out`. Jackknif residual, knowing as $t_i=\frac{\hat{y}_{(i)}-\hat{y_i}}{\hat{\sigma}_{(i)}(1+x^{'}_{i}(X^{'}_{(i)}X_{(i)})^{-1}x_{i})^{-1/2}}$ and cook distance used this idea.


# Outlier detection in quantile regression

Due to the robust property of quantiles, quantile regressions are relatively robust towards outliers than mean regression model, This property has been discussed by Onyedikachi(2015). They used influence funtion to discuss the property of robustness of quantile regression. 

Suppose $T$ is a function of $F$, the influence function is the directional derivative of $T(F)$ at $F$, and it measures the effect of a small perturbation in $F$ on $T(F)$. For the $\tau$th quantile points, influence function can be expressed as,

\begin{equation}

IF(y;T;F)=\left\{
\begin{aligned}
\frac{\tau}{f(F^{-1}(\tau))} & ; & y > F^{-1}(\tau) \\
\frac{(\tau-1)}{f(F^{-1}(\tau))} & ; & y \leq F^{-1}(\tau) 
\end{aligned}
\right.

(\#eq:quantile_influence)
\end{equation}

The influence function \@ref(eq:quantile_influence) indicate that the contamination in $y$ on quantile points is bounded. Onyedikachi(2015) also provided the influence function for quantile regression.

Let $F$ represent the joint distribution of the pairs $(x,y)$, we have,

\begin{equation}
IF((y,x),\hat{\beta}_{F(\tau)},F)=Q^{-1}xsgn(y-x^{'}\hat{\beta}_{F}(\tau))
(\#eq:quantile_regression_influence)
\end{equation}

where 

\begin{equation}
dF=dG(x)f(y|x)dy
(\#eq: dg)
\end{equation}

\begin{equation}
Q=\int xx^{'}f(X^{'}\hat{\beta}_{F}(\tau))dG(x)
(\#eq: q_influence)
\end{equation}

\@ref(eq:quantile_regression_influence) implies that in quantile regression, the quantile regression estimates will not be affected by any chagne in the value of the dependent variable for some observations as long as the relative positions of the observation points to the fitted plane are maintained. 

## Displaying how do the outliers affect quantile regression

We conducted simulation to display how does outliers affect quantile regression estimations. In two simple simulation studies, we generate 100 sample observation and 3 outliers. The outliers are distributed in two locations in each case. We fitted mean regression and quantile regression based on these dataset to observe how do outliers affect model coefficients. 


```{r, eval=TRUE, echo=FALSE, fig.align="center", fig.height = 3, fig.width = 8,fig.align = "center", fig.cap = "Fitting quantile regression model on quantile 0.1, 0.5 and 0.9 using simulated datasets with and without outliers. The outliers located at the top-left of the original dataset. Results show that outliers pull up the slope of the 0.9 and 0.1 regression line. When outliers located at the bottom-right of the original dataset, results show that outliers pull down the slope of the 0.1 regression line."}

x <- sort(runif(100))
y1 <- 40*x + x*rnorm(100, 0, 10)
df <- data.frame(y1, x)
add_outlier <- data.frame(y1 = c(60,61,62), x = c(0.71, 0.73,0.75))
df_o <- rbind(df, add_outlier)

coef1 <- rq(y1 ~ x, tau = c(0.1, 0.5, 0.9), data = df, method = "br")$coef
rq_coef1 <- data.frame(intercept = coef1[1, ], coef = coef1[2, ],
                       tau_flag =colnames(coef1))

coef2 <- rq(y1 ~ x, tau = c(0.1, 0.5, 0.9),data = df_o, method = "br")$coef
rq_coef2 <- data.frame(intercept = coef2[1, ], coef = coef2[2, ],
                       tau_flag =colnames(coef2))
p1 <- ggplot(df_o) +
        geom_point(aes(x = x, y = y1), alpha = 0.1) +
        geom_abline(data = rq_coef1, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))+
        geom_abline(data = rq_coef2, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))

#####
x <- sort(runif(100))
y2 <- 40*x + x*rnorm(100, 0, 10)
df <- data.frame(y2, x)
add_outlier <- data.frame(y2 = c(1,2,3), x = c(0.71, 0.73,0.75))
df_o <- rbind(df, add_outlier)

coef1 <- rq(y2 ~ x, tau = c(0.1, 0.5, 0.9), data = df, method = "br")$coef
rq_coef1 <- data.frame(intercept = coef1[1, ], coef = coef1[2, ], tau_flag = colnames(coef1))

coef2 <- rq(y2 ~ x, tau = c(0.1, 0.5, 0.9), data = df_o, method = "br")$coef
rq_coef2 <- data.frame(intercept = coef2[1, ], coef = coef2[2, ], tau_flag = colnames(coef2))

p2 <- ggplot(df_o) +
        geom_point(aes(x = x, y = y2), alpha = 0.1) +
        geom_abline(data = rq_coef1, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))+
        geom_abline(data = rq_coef2, aes(intercept = intercept,
                                   slope = coef, colour = tau_flag))

grid.arrange(p1, p2, nrow = 1)
```

We also change the location of outliers or add outliers numbers to observe how they affect coeficient estimations on each quantile. These simulation studies are extended to multi-variables model.Using the simulated data to construct quantile regression model. By comparing the four models, we have a brief idea of the effect of outliers locating. The results show that when outliers moving down in y direction for 10 unit, it pulls down the slope on every quantile(comparing the result of model rq(y1~x) and rq(y2~x)). However, keeping moving down the outliers does no change to the slopes. This reflect the boundary theory expressed by influence function above.

```{r move-y1, warning=FALSE,message=FALSE, eval = TRUE, echo = FALSE, fig.height = 4, fig.width = 10, fig.align = "center", fig.cap = "Left fig: Fitting quantile regression models using simulated data. We keep moving down the outliers in y direction in y2 (y-5), y3 (y-10) and y4 (y-15). Right fig: Fitting quantile regression models using simulated data. We keep moving down the outliers in y direction getting datasets with variable y2 (=y-5), y3 (=y-10) and y4 (=y-15). Results show that in single predictor case, outliers moving down in y make no difference to the quantile regression coefficients estimations"}

x <- sort(runif(100))
y <- 40*x + x*rnorm(100, 0, 10)
selectedX <- sample(50:100,5)
y2<- y
y2[selectedX] <- x[1:5]*rnorm(5, 0, 10)
y3 <- y2
y3[selectedX] <- y2[selectedX] - 10
y4 <- y3
y4[selectedX] <- y3[selectedX] - 10
df <- data.frame(x, y, y2, y3, y4)
df_m <- df %>% gather(variable, value, -x)
p1 <- ggplot(df_m, aes(x = x, y=value)) +
        geom_point(alpha = 0.5) +
        xlab("x") +
        ylab("y") +
        facet_wrap(~variable, ncol=2, scale = "free_y") +
        geom_quantile(quantiles = seq(0.1, 0.9, 0.1), colour =                          "purple") +
        geom_smooth(method = "lm", se = FALSE, colour = "orange")

coefs <- 2:5 %>%
  map(~ rq(df[, .] ~ x, data = df, seq(0.1, 0.9, 0.1))) %>%
  map_df(~ as.data.frame(t(as.matrix(coef(.)))))
colnames(coefs) <- c("intercept", "slope")
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")

df_m1 <- data.frame(model, tau, coefs)
df_mf <- df_m1 %>% gather(variable, value, -c(model, tau))
p2 <- ggplot(df_mf, aes(x = tau, y = value, colour = model)) +
        geom_point() +
        geom_line() +
        facet_wrap(~ variable, scale = "free_y") +
        xlab('quantiles') +
        ylab('coefficients')

grid.arrange(p1, p2, nrow = 1)
```

We also observed the change of coefficients in multi-variable model. The results show that coefficients changes slowly when keep moving down the outliers in y-direction.

```{r move-y-multi1, eval=TRUE, echo=FALSE,warning=FALSE,message=FALSE, fig.height = 3, fig.width = 8, fig.align = "center", fig.cap = "Fitting quantile regression models using simulated data. We keep moving down the outliers in y direction getting three datasets with different locations of outliers (changing in y-aixs, y2 (=y-5), y3 (=y-10) and y4 (=y-15)). Results show that in multi predictors case, outliers moving down in y make small change to the quantile regression coefficients estimations"}

n <- 100
set.seed(101)
x1 <- sort(rnorm(n, 0, 1))
x2 <- sort(rnorm(n, 1, 2))
y <- 40*(x1 + x2) + x1*rnorm(100, 0, 10) + x2*rnorm(100, 0, 10)
selectedX <- sample(50:100,5)
y2<- y
y2[selectedX] <- x1[1:5]*rnorm(5, 0, 10) + x2[1:5]*rnorm(5, 0, 10)
y3 <- y2
y3[selectedX] <- y2[selectedX] - 100
y4 <- y3
y4[selectedX] <- y3[selectedX] - 100
df <- data.frame(y, y2, y3, y4, x1, x2)
coefs <- 1:4 %>%
  map(~ rq(df[, .] ~ x1 + x2, data = df, seq(0.1, 0.9, 0.1))) %>%
  map_df(~ as.data.frame(t(as.matrix(coef(.)))))
colnames(coefs) <- c("intercept", "slope_x1", "slope_x2")
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(model, tau, coefs)
df_mf <- df_m1 %>% gather(variable, value, -c(model, tau))
ggplot(df_mf, aes(x = tau, y = value, colour = model)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ variable, scale = "free_y") +
  xlab('quantiles') +
  ylab('coefficients')

```

If moving outliers in same pattern moving on x direction, slopes change every time outlier moves. To go further, each move does different effect on different quantiles.

```{r move-x1, eval=TRUE, echo=FALSE, warning=FALSE,message=FALSE, fig.height = 4, fig.width = 10, fig.align = "center", fig.cap = "Left fig: Fitting quantile regression models using simulated data. We keep moving the outliers to the right in x direction getting three datasets with different locations of outliers (changing in x-aixs, x2 (=x+0.2), x3 (=x+0.4) and x4 (=x+0.6)). Right fig: Fitting quantile regression models using simulated data. We keep moving the outliers to the right in x direction getting three datasets with different locations of outliers (changing in x-aixs, x2 (=x+0.2), x3 (=x+0.4) and x4 (=x+0.6)).Results show that in single predictors case, outliers moving right in x make significant change to the quantile regression coefficients estimations."}

x <- sort(runif(100))
y <- 40*x + x*rnorm(100, 0, 10)
selectedIdx <- sample(50:100,5)
df <- data.frame(y)
df$y2 <- y
df$x <- x
df$y2[selectedIdx] <- df$x[1:5]*rnorm(5, 0, 10)
df$x2 <- x
df$x2[selectedIdx] <- df$x2[selectedIdx] + 0.2
df$x3 <- df$x2
df$x3[selectedIdx] <- df$x3[selectedIdx] + 0.2
df$x4 <- df$x3
df$x4[selectedIdx] <- df$x4[selectedIdx] + 0.2
df_m <- df %>% gather(variable, value, -y, -y2)
p1 <- ggplot(df_m, aes(x = value, y=y2)) +
  geom_point() +
  xlab("x") +
  ylab("y") +
  facet_wrap(~variable, ncol=2, scale = "free") +
  geom_quantile(quantiles = seq(0.1, 0.9, 0.1))

coefs <- 3:6 %>%
  map(~ rq(df$y2 ~ df[, .], data = df, seq(0.1, 0.9, 0.1))) %>%
  map_df(~ as.data.frame(t(as.matrix(coef(.)))))
colnames(coefs) <- c("intercept", "slope")
tau <- rep(seq(0.1, 0.9, by = 0.1), 4)
model <- paste('rq', rep(1:4, each = 9), sep="")
df_m1 <- data.frame(model, tau, coefs)
df_mf <- df_m1 %>% gather(variable, value, -c(model, tau))
p2 <- ggplot(df_mf, aes(x = tau, y = value, colour = model)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ variable, scale = "free_y") +
  xlab('quantiles') +
  ylab('coefficients')

grid.arrange(p1, p2, nrow = 1)
```

In conclusion, quantile regression response differently to outliers comparing mean regression in two aspects: (a) not all models on each quantile will be affected when outliers exist. If we are interested in model on particular quantile, the effect of outliers should be carefully considered. (b) quantile regression model do not have robustness properties to so called leverage points. 

# Outlier diagnosting methods for quantile regression 

We proposed three methods for quantile regression outlier diagnostic which will be discussed as follows.

* Standard residual-Robust Distance

Leverage points can not be detected using the famous "Hat Matrix" in quantile regression since the coefficient estimation of quantile regression do not satisfy $\hat{\beta}=(X^{'}X)^{-1}X^{'}Y$. One way to identify possible multivariate outliers is to calculate a distance from each point to a "center" of the data. An outlier would then be a point with a distance larger than some predetermined cutoff. A conventional measurement of quadradic distance from a point $X$ to a location $Y$ given given a shape $S$, in the multivariate setting is:

$$d^{2}_{S}(X,Y)=(X-Y)^{'}S^{-1}(X-Y)$$

This distance is often called Mahalanobis squared distance(MSD). If there are only a few outliers, large values of $d^{2}_{S}(x_i, \bar{X})$, when $\bar{X}$ and $S$ are the standard sample mean and covariance matrix, indicate that the point $x_i$ is an outlier(Barnett and Lewis 1994). The distribution of the MSD with both the true location and shape parameters and the standard sample location and shape parameters is well known(Gnanadesikan and Kettenring 1972). However, the standard sample locatoin and shape parameters are not robust to outliers, and the distributional fit to the distance breaks down when robust measures of location and shape are used in the MSD(Rousseeuw and van Zomeren 1991). Datasets with multiple outliers or clusters of outliers are subject to problems of masking and swamping(Pearson and Chandra Sekar 1936).

Problems of masking and swamping can be resolved by using robust estimates of shape and location, which by definition are less affected by outliers. We use Rousseeuw's minimum covariance determinant(MCD)(Rousseeuw 1985) to estimate the location and shape of the data. 

The MCD estimator is a robust, high-breakdown point which can be defined as:

$$MCD = (\bar{X}^{*}_{h}, S^{*}_{h})$$
where $h={p: |S^{*}_{h}|<|S^{*}_{k}|,|k|=p}$, $\bar{X}^{*}_{h}=\frac{1}{p}\sum_{i \in p}x_{i}$, $S^{*}_{p}=\frac{1}{p}\sum_{i \in p}(x_i-\bar{X}^{*}_{p})(x_i-\bar{X}^{*}_{p})^{'}$.

The value $p$ can be thought of as the minimum number of points which must not be outliers. The MCD has its highest possible breakdown at $h=[\frac{n+p+1}{2}]$ where $[.]$ is the greatest integer function. Because we are interested in outlier detection, we will use $h$ at its highest possible breakdown. $h=[\frac{n+p+1}{2}]$ in our calculations, and we refer to a sample of size $h$ as a "half sample" The MCD is omputed from the "closet" half sample, and therefore, the outlying points will have little affect on the MCD location or shape estimate.

We use MCD to detect outliers in covariates and use absolute standard residuals to detect outliers in y direction.
 
* Generalized Cook Distance

To assess the influence of the $i$th case on the coefficient estimation of quantile regression, we compare the difference between $\hat{\theta}_{[i]}$ and $\hat{\theta}$.

Case-deletion is a classical approach to study the effects of dropping the $i$th observation deleted. Thus, the complete-data log-likelihood function based on the data with the $i$th case deleted will be denoted by $L_{c}(\theta|y_{c[i]})$. Let $\hat{\theta}_{[i]}=(\hat{\beta}^{T}_{p[i]}, \hat{\sigma}^{2}_{[i]})^{T}$ be the maximizer of the function $Q_{[i]}(\theta|\hat{\theta})=E_{\hat{\theta}}[l_{c}(\theta|Y_{c[i]})|y]$, where $\hat{\theta}=(\hat{\beta}^{T}, \hat{\sigma}^{2})^{T}$ is the ML estimate of $\theta$. 

To calculate the case-deletion estimate $\hat{\theta}_{[i]}$ of $\theta$, proposed the following one-step approximation based on Q-function,

$$\hat{\theta}_{[i]}=\hat{\theta}+\{-Q(\hat{\theta}|\hat{\theta})\}^{-1}Q_{[i]}(\hat{\theta}|\hat{\theta})$$

where

$$Q(\hat{\theta}|\hat{\theta})=\frac{\partial^{2}Q(\theta|\hat{\theta})}{\partial\theta\partial \theta^{T}}|_{\theta=\hat{\theta}}$$

$$Q_{[i]}(\hat{\theta}|\hat{\theta})=\frac{\partial Q_{[i]}(\theta|\hat{\theta})}{\partial\theta}|_{\theta=\hat{\theta}}$$


are the Hessian matrix and the gradient vector evaluated at $\hat{\theta}$, respectively.

For measuring the distance between $\hat{\theta}_{[i]}$ and $\hat{\theta}$. We consider generalized cook distance as follows.

$$GD_{i} =(\hat{\theta}_{[i]}-\hat{\theta})^{T}\{-Q(\hat{\theta}|\hat{\theta})\}(\hat{\theta}_{[i]}-\hat{\theta}), i=1,...,n$$


* Q-function Distance

The measurement of the influence of the $i$th case is based on the Q-distance function, similar to the likelihood distance $LD_{i}$ which was defined as

$$QD_{i}=2\{Q(\hat{\theta}|\hat{\theta})-Q(\hat{\theta}_{[i]}|\hat{\theta})\}$$

* Mean Post Probability 

Baysian quantile regression added a latent variable $v_i$ into model for each observation. Every $v_i$ is assumed to have an exponential distribution with mean $\sigma$, that with the likelihood produces a posterior distributed according to a generalized inverse Gaussian with parameters.

If we define the variable $O_i$, which takes value equal to 1 when the $i$th observation is an outlier, and 0 otherwise. Then we propose to calculate the pro
bility of an observation being an outlier as

$$P(O_i=1)=\frac{1}{n-1}\sum_{j \neq i}P(v_i > v_j|data)$$

The probability in the expression above can be approximated given the
MCMC draws, as follows:

$$P(O_i = 1)=\frac{1}{M}I(v^{(l)_i}>max_{k \in 1:M}v^{(k)}_j)$$

where $M$ is the size of the chain of $v_i$ after the burn-in perior and $v^{(l)}_i$ is the $l$th draw of this chain.


* K-L Divergence

Similar with mean posterior probability method, we also caculates Kullback-Leibler divergence which proposed by Kullback and Leibler(1951) as a more precise method of measuring the distance between those latent variables in Bayes quantile regression. The Kullback-Leibler divergence is defined as:
  
$$K(f_i, f_j)=\int log(\frac{f_i(x)}{f_j(x)}f_{i}(x))dx$$
  
where $f_i$ could be the posterior conditional distribution of $v_i$ and $f_j$ the posterior conditional distribution of $v_j$. We should average this divergence for one observation based on the distance from all others,
  
$$KL(f_i)=\frac{1}{n-1}\sum_{j\neq i}K(f_i, f_j)$$
  
The outliers should show a high probability value for this divergence. We compute the integral using the trapezoidal rule.

# Examining outlier detection

We developed R package `quokar` to implete quantile regression outlier diagnostic methods. This package mainly realized two basic features: (a) plot the outlier states; (b) plot data with outliers marked. `quokar` is available from Github at https://github.com/wenjingwang/quokar, so to install and load withn R use:

```{r}
devtools::install_github("wenjingwang/quokar")
library(quokar)
```

We implete ais data as an example to introduce this package. AIS data include 14 variables for 100 female atheletes.

## Plot the outlier stats

In single variable case, we can use scatter plot to represent the outlier stats. The following code showed how to display suspicious outliers based on quantile regression models.

```{r, warning=FALSE,message=FALSE, fig.height=3, fig.width=5, fig.align = "center", fig.cap="Plot the outlier stats."}
data(ais)
ais_female <- filter(ais, Sex == 1)
case <- 1 : nrow(ais_female)
ais_female <- cbind(case, ais_female)
coef_rq <- coef(rq(BMI ~ LBM, tau = c(0.1, 0.5, 0.9),
                   data = ais_female, method = "br"))

br_coef <- data.frame(intercept = coef_rq[1, ],
                      coef = coef_rq[2, ],
                      tau_flag = colnames(coef_rq))
ggplot(ais_female)+
  geom_point(aes(x = LBM, y = BMI)) +
  geom_abline(data = br_coef, aes(intercept = intercept,
                                  slope = coef,
                                  colour = tau_flag), size = 1) +
  geom_text(data = subset(ais_female, case %in% c(1, 75)),
                          aes(x = LBM, y = BMI, label = case), 
            colour = "red",hjust = 0, vjust = 0) +
  scale_colour_brewer("Dark2") +
  theme_dark()

```

## Plot data with outliers marked

Scatter plot has limitations when tackling multi-variable regression cases. In `quokar`, we provide functions to do outlier diagnostic which return the dataframe easily to plot data with outliers marked.

* residual-robust distance method

First, we calculate residuals, mahananobi distance and robust distance for quantile regression using function `plot_distance`. Simutaneously, it provides the cutoff value for identifying the outliers in regression models.

```{r}
tau <- c(0.1, 0.5, 0.9)
object <- rq(BMI ~ LBM + Bfat, data = ais_female, tau = tau)
plot_distance <- frame_distance(object, tau = c(0.1, 0.5, 0.9))
distance <- plot_distance[[1]]
head(distance, 3)
cutoff_v <- plot_distance[[2]]; cutoff_v
cutoff_h <- plot_distance[[3]]; cutoff_h
```

Function `plot_distance` returns the tidy data form for plotting data with outliers marked and overlaying the cutoff lines. 

```{r, warning=FALSE,message=FALSE, fig.height=3, fig.width=8, fig.align = "center", fig.cap="Robust Distance-Residual Plot. Points on the right of vertical cutoff line are considered leverage points and points above the horizental cutoff line are outliers in y-direction."}

n <- nrow(object$model)
case <- rep(1:n, length(tau))
distance <- cbind(case, distance)
distance$residuals <- abs(distance$residuals)
tau_flag <- paste("tau", tau, sep="")
text_flag <- 1:length(cutoff_h) %>%
                            map(function(i){
                            distance %>% 
                            filter((residuals > cutoff_h[i] |rd > cutoff_v)
                                & tau_flag == tau_flag[i])})
##need polish
text_flag_d <- rbind(text_flag[[1]], text_flag[[2]], text_flag[[3]])
ggplot(distance, aes(x = rd, y = residuals)) +
      geom_point() +
      geom_hline(data = data.frame(tau_flag, cutoff_h),   
                 aes(yintercept = cutoff_h), colour = "red") +
      geom_vline(xintercept = cutoff_v, colour = "red") +
      geom_text(data = text_flag_d, aes(label = case), hjust = 0, vjust = 0) +
      facet_wrap(~ tau_flag, scales = 'free_y') +
      xlab("Robust Distance") +
      ylab("|Residuals|")
```

* Generalized cook distance and Q function distance

We apply generalized cook distance and Q function distance methods in function `frame_mle`. This function returns generalized cook or q function distance for regression model on each given quantile. The results are also in tidy data structure which can be easily used for plotting the two distances with outliers marked.


```{r,warning=FALSE,message=FALSE, results = "hide", fig.height=3, fig.width=8, fig.align = "center", fig.cap="Generalized cook distance of each observation on quantile 0.1, 0.5 and 0.9. Case 75 has relative large cook distance-funtion distance to other points"}
y <- ais_female$BMI
x <- cbind(1, ais_female$LBM, ais_female$Bfat)
case <- rep(1:length(y), length(tau))
GCD <- frame_mle(y, x, tau, error = 1e-06, iter = 10000,
                  method = 'cook.distance')
GCD_m <- cbind(case, GCD)
ggplot(GCD_m, aes(x = case, y = value )) +
    geom_point() +
    facet_wrap(~variable, scale = 'free_y') +
    geom_text(data = subset(GCD_m, value > mean(value) + 2*sd(value)),
              aes(label = case), hjust = 0, vjust = 0) +
    xlab("case number") +
    ylab("Generalized Cook Distance")
```

The same, visualization of Q function diagnostic results are shown in fig,

```{r,warning=FALSE,message=FALSE, results = "hide", fig.height=3, fig.width=8, fig.align = "center", fig.cap="Q function distance of each observation on quantile 0.1, 0.5 and 0.9. Case 75 has relative large Q function distance to other points"}

QD <- frame_mle(y, x, tau, error = 1e-06, iter = 10000,
               method = 'qfunction')
QD_m <- cbind(case, QD)
ggplot(QD_m, aes(x = case, y = value)) +
 geom_point() +
 facet_wrap(~variable, scale = 'free_y')+
 geom_text(data = subset(QD_m, value > mean(value) + sd(value)),
           aes(label = case), hjust = 0, vjust = 0) +
 xlab('case number') +
 ylab('Qfunction Distance')
```

Same as above, we also applied mean post probability, KL divergence to diagnose,

```{r,warning=FALSE,message=FALSE, fig.height=3, fig.width=8, fig.align = "center", fig.cap="Mean posterior probability of each case on quantile 0.1, 0.5 and 0.9. The mean posterior probabilities are calculated based on the postierior distribution of latent variable using Bayesian quantile regression method"}

y <- ais_female$BMI
x <- matrix(c(ais_female$LBM, ais_female$Bfat), ncol = 2, byrow = FALSE)
tau <- c(0.1, 0.5, 0.9)
case <- rep(1:length(y), length(tau))
prob <- frame_bayes(y, x, tau, M =  100,
                 method = 'bayes.prob')

prob_m <- cbind(case, prob)
ggplot(prob_m, aes(x = case, y = value )) +
   geom_point() +
   facet_wrap(~variable, scale = 'free') +
  geom_text(data = subset(prob_m, value > mean(value) + 2*sd(value)),
            aes(label = case), hjust = 0, vjust = 0) +
   xlab("case number") +
   ylab("Mean probability of posterior distribution")

```

```{r,warning=FALSE,message=FALSE, fig.height = 3, fig.width = 8, fig.align = "center", fig.cap = "Kullback and Leibler divergence of each case on quantile 0.1, 0.5 and 0.9. The Kullback-Leibler divergence is calculated based on the postierior distribution of latent variable using Bayesian quantile regression method."}

kl <- frame_bayes(y, x, tau, M = 100,
                  method = 'bayes.kl')
kl_m <- cbind(case, kl)
ggplot(kl_m, aes(x = case, y = value)) +
  geom_point() +
  facet_wrap(~variable, scale = 'free')+
  geom_text(data = subset(kl_m, value > mean(value) + 2*sd(value)),
            aes(label = case), hjust = 0, vjust = 0) +
  xlab('case number') +
  ylab('Kullback-Leibler')
```

# Visualizing quantile regression

Visualization of quantile regression will help us understand the questions 'How does the shape of the model compare to the shape of the data?'. In addition, we can have a good impression of the location of models on each quantile. We use GGobi to visualize quantile regression model.

## Linear quantile regression model

In two predictors case, quantile regression models are lines in space. We use ais data fitting models and visualize them with GGobi.

```{r, eval=TRUE, echo=TRUE, fig.align="center",out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model-single-pixel/linear-3D-1.png",
                          "Figures/QR-model-single-pixel/linear-3D-2.png",
                          "Figures/QR-model-single-pixel/linear-3D-3.png",
                          "Figures/QR-model-single-pixel/linear-3D-4.png",
                          "Figures/QR-model-single-pixel/linear-3D-5.png",
                          "Figures/QR-model-single-pixel/linear-3D-6.png",
                          "Figures/QR-model-single-pixel/linear-3D-7.png",
                          "Figures/QR-model-single-pixel/linear-3D-8.png",
                          "Figures/QR-model-single-pixel/linear-3D-9.png"))
```

In three predictor case, quantile regression models are cuboids in space which were displayed as follows,

```{r, eval=TRUE, echo=TRUE, fig.align="center", out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model-single-pixel/linear-4D-1.png",
                          "Figures/QR-model-single-pixel/linear-4D-2.png",
                          "Figures/QR-model-single-pixel/linear-4D-3.png",
                          "Figures/QR-model-single-pixel/linear-4D-4.png",
                          "Figures/QR-model-single-pixel/linear-4D-5.png",
                          "Figures/QR-model-single-pixel/linear-4D-6.png",
                          "Figures/QR-model-single-pixel/linear-4D-7.png",
                          "Figures/QR-model-single-pixel/linear-4D-8.png",
                          "Figures/QR-model-single-pixel/linear-4D-9.png"))
```

## Non-linear quantile regression model

In non-linear case, we use elliptic hyperboloid and hyperbolic paraboloid as examples.

```{r, eval=TRUE, echo=TRUE, fig.align="center", out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model/curve1-1.png",
                          "Figures/QR-model/curve1-2.png",
                          "Figures/QR-model/curve1-3.png",
                          "Figures/QR-model/curve1-4.png",
                          "Figures/QR-model/curve1-5.png",
                          "Figures/QR-model/curve1-6.png",
                          "Figures/QR-model/curve1-7.png",
                          "Figures/QR-model/curve1-8.png",
                          "Figures/QR-model/curve1-9.png"))
```

```{r, include = FALSE, out.width = '20%', out.height = '20%'}
knitr::include_graphics(c("Figures/QR-model/curve2-1.png",
                          "Figures/QR-model/curve2-2.png",
                          "Figures/QR-model/curve2-3.png",
                          "Figures/QR-model/curve2-4.png",
                          "Figures/QR-model/curve2-5.png",
                          "Figures/QR-model/curve2-6.png"))
```

# Future work

high-dimensional and extreme quantile work.

# Reference

Koenker R, Machado J A F. Goodness of fit and related inference processes for quantile regression[J]. Journal of the american statistical association, 1999, 94(448): 1296-1310.

Fitzenberger B. The moving blocks bootstrap and robust inference for linear least squares and quantile regressions[J]. Journal of Econometrics, 1998, 82(2): 235-287.

Chernozhukov V, Hansen C. Instrumental variable quantile regression: A robust inference approach[J]. Journal of Econometrics, 2008, 142(1): 379-398.

Geraci M, Bottai M. Quantile regression for longitudinal data using the asymmetric Laplace distribution[J]. Biostatistics, 2007, 8(1): 140-154.

Koenker R. Quantile regression for longitudinal data[J]. Journal of Multivariate Analysis, 2004, 91(1): 74-89.

Korobilis D. Quantile regression forecasts of inflation under model uncertainty[J]. International Journal of Forecasting, 2017, 33(1): 11-20.

Autor D H, Houseman S N, Kerr S P. The Effect of Work First Job Placements on the Distribution of Earnings: An Instrumental Variable Quantile Regression Approach[J]. Journal of Labor Economics, 2017, 35(1): 149-190.


Mitchell J A, Dowda M, Pate R R, et al. Physical Activity and Pediatric Obesity: A Quantile Regression Analysis[J]. Medicine and science in sports and exercise, 2017, 49(3): 466.

Gallego-√Ålvarez I, Ortas E. Corporate environmental sustainability reporting in the context of national cultures: A quantile regression approach[J]. International Business Review, 2017, 26(2): 337-353.

Maciejowska K, Nowotarski J, Weron R. Probabilistic forecasting of electricity spot prices using Factor Quantile Regression Averaging[J]. International Journal of Forecasting, 2016, 32(3): 957-965.


Parente P M D C, Santos Silva J. Quantile regression with clustered data[J]. Journal of Econometric Methods, 2016, 5(1): 1-15.

Galvao A F, Kato K. Smoothed quantile regression for panel data[J]. Journal of Econometrics, 2016, 193(1): 92-112.

Arellano M, Bonhomme S. Nonlinear panel data estimation via quantile regressions[J]. The Econometrics Journal, 2016, 19(3).

Canay I A. A simple approach to quantile regression for panel data[J]. The Econometrics Journal, 2011, 14(3): 368-386.

Geraci M. Linear quantile mixed models: the lqmm package for Laplace quantile regression[J]. Journal of Statistical Software, 2014, 57(13): 1-29.

Chernozhukov V, Hansen C. Instrumental quantile regression inference for structural and treatment effect models[J]. Journal of Econometrics, 2006, 132(2): 491-525.